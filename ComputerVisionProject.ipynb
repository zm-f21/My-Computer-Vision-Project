{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BCBLWEHPxeW"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zm-f21/My-Computer-Vision-Project/blob/Branch-4-Smaller-Dataset/ComputerVisionProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b> Object Detection Using YOLO <b> </h1></center>"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is designed to provide a comprehensive understanding of how to use YOLO, a state-of-the-art method in computer vision, for detecting objects in images.\n",
        "\n",
        "Object detection and classification is a key technology in many areas, such as automated vehicles, security, and even healthcare.\n",
        "\n",
        "We will begin with the basics of preparing (pre processing) an image dataset, ensuring it is ready for effective model training.We will then explore how YOLO, a type of convolutional neural network, automatically extracts features from images to recognize different objects. Understanding this process is crucial for grasping how YOLO operates.\n",
        "\n",
        "The core of this tutorial is focused on transfer learning using YOLO. We will teach you how to take a pre-trained YOLO model and adapt it to a new dataset. This technique is efficient and powerful, allowing us to harness the strengths of YOLO with less computational effort.\n",
        "\n",
        "By the end of this tutorial, you will have hands-on experience with preparing data, implementing YOLO, and understanding the principles behind it. This tutorial aims to equip students with practical skills and knowledge in one of the most exciting fields in technology."
      ],
      "metadata": {
        "id": "ZIi4bjh7mIJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since its inception, the YOLO family of object detection models has come a long way. YOLOv8 is the most recent addition to this famous anchor-based single-shot family of object detectors. It comes with a bunch of improvements which include state-of-the-art accuracy and speed.  In this article, we will be fine tuning the YOLOv8 object detection model on a real-world pothole detection dataset."
      ],
      "metadata": {
        "id": "vsmlgXapu5rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous lecture, you were asked to make your own custom datasets for a project you want to work on. Today we will explore how to finetune YOLO on a certain dataset.  "
      ],
      "metadata": {
        "id": "v5zL0avFnNaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "rr-j-jCmaHId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources:\n",
        "# Yolo Classification Model: https://docs.ultralytics.com/tasks/classify/#val"
      ],
      "metadata": {
        "id": "-4UGKW3X7ja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run the code below when you want to clear files:\n",
        "#!rm -rf /content/*"
      ],
      "metadata": {
        "id": "GNNIiYig7l6t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create a folder for the unzipped files\n",
        "os.makedirs(\"/content/Styles_unzipped\", exist_ok=True)\n",
        "\n",
        "# Unzip the file into that folder\n",
        "\n",
        "with zipfile.ZipFile(\"/content/Styles.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/Styles_unzipped\")\n",
        "\n",
        "# Verify contents\n",
        "os.listdir(\"/content/Styles_unzipped\")\n"
      ],
      "metadata": {
        "id": "Qeb4_GgaaiRZ",
        "outputId": "f4120388-b81f-44fd-c33c-5ef4c544586c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cartoon Stylized', '__MACOSX', 'Vector Art', 'Pixel Art', 'Hand Painted']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import shutil\n",
        "import os, random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# #/\n",
        "# base_dir = \"/content/asset_styles\"\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Paths for train/val/test\n",
        "# for split in [\"train\", \"val\", \"test\"]:\n",
        "#     os.makedirs(os.path.join(base_dir, split), exist_ok=True)\n",
        "#     /#\n"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sympy==1.12\n",
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U ultralytics"
      ],
      "metadata": {
        "id": "2eKqlzpkTad_",
        "outputId": "1c00b414-24ab-4b2e-8c45-a8024c4f0152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0 requires sympy>=1.13.3, but you have sympy 1.12 which is incompatible.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.14.0\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.222)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# OLD CODE OCT 26:\n",
        "# The Code Below is the same as the Code Above but it resizes large images to\n",
        "# avoid bomb decrompression and to convert tranparent images into RGB\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "source_dir = \"/content/Styles_unzipped\"   # parent folder\n",
        "output_base = \"/content/asset_styles_yolo\"       # YOLO-ready dataset output\n",
        "splits = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "augment = True  # enable data augmentation\n",
        "\n",
        "# --- Step 1: Create split folders ---\n",
        "for split in splits.keys():\n",
        "    os.makedirs(os.path.join(output_base, split), exist_ok=True)\n",
        "\n",
        "# --- Step 2: Loop through style folders (Cartoon Stylized, Pixel Art, Vector Art) ---\n",
        "style_folders = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]\n",
        "\n",
        "for style in style_folders:\n",
        "    style_path = os.path.join(source_dir, style)\n",
        "    label_name = style.replace(\" \", \"_\")  # YOLO doesn’t like spaces in class names\n",
        "\n",
        "    # Create subfolders for each split\n",
        "    for split in splits.keys():\n",
        "        os.makedirs(os.path.join(output_base, split, label_name), exist_ok=True)\n",
        "\n",
        "    # Collect all image paths from all subfolders (Background, Characters, Object, UI, etc.)\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(style_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                image_paths.append(os.path.join(root, f))\n",
        "\n",
        "    print(f\"📂 Found {len(image_paths)} images for {label_name}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(image_paths)\n",
        "    n = len(image_paths)\n",
        "    train_end = int(splits[\"train\"] * n)\n",
        "    val_end = int((splits[\"train\"] + splits[\"val\"]) * n)\n",
        "\n",
        "    split_data = {\n",
        "        \"train\": image_paths[:train_end],\n",
        "        \"val\": image_paths[train_end:val_end],\n",
        "        \"test\": image_paths[val_end:]\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Copy and augment images ---\n",
        "    for split, imgs in split_data.items():\n",
        "        dest_dir = os.path.join(output_base, split, label_name)\n",
        "        for img_path in tqdm(imgs, desc=f\"{label_name} → {split}\"):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    # --- Fix transparency / color mode ---\n",
        "                    if img.mode == \"RGBA\":\n",
        "                        img = img.convert(\"RGB\")\n",
        "\n",
        "                    # --- Resize if image is very large ---\n",
        "                    if max(img.size) > 2048:\n",
        "                        img.thumbnail((2048, 2048))  # keeps aspect ratio\n",
        "\n",
        "                    base_name = os.path.basename(img_path)\n",
        "                    save_path = os.path.join(dest_dir, base_name)\n",
        "                    img.save(save_path)\n",
        "\n",
        "                    # --- Step 4: Data Augmentation (optional) ---\n",
        "                    if augment and split == \"train\":\n",
        "                        # Horizontal Flip\n",
        "                        flipped = ImageOps.mirror(img)\n",
        "                        flipped.save(os.path.join(dest_dir, \"flip_\" + base_name))\n",
        "\n",
        "                        # Vertical Flip\n",
        "                        flipped_v = ImageOps.flip(img)\n",
        "                        flipped_v.save(os.path.join(dest_dir, \"vflip_\" + base_name))\n",
        "\n",
        "                        # Small Rotation (-15 to +15 degrees)\n",
        "                        angle = random.uniform(-15, 15)\n",
        "                        rotated = img.rotate(angle, expand=True)\n",
        "                        rotated.save(os.path.join(dest_dir, f\"rot{int(angle)}_\" + base_name))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Dataset prepared successfully in YOLO classification format!\")\n",
        "print(f\"📁 Output location: {output_base}\")\n"
      ],
      "metadata": {
        "id": "8zZfEJ-GlKJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fb8479-e803-4f97-a7f4-9c2db76e4e5f",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Hand_Painted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted → train: 100%|██████████| 70/70 [01:51<00:00,  1.60s/it]\n",
            "Hand_Painted → val: 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]\n",
            "Hand_Painted → test: 100%|██████████| 11/11 [00:03<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Pixel_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art → train: 100%|██████████| 70/70 [00:08<00:00,  7.92it/s]\n",
            "Pixel_Art → val: 100%|██████████| 19/19 [00:00<00:00, 33.05it/s]\n",
            "Pixel_Art → test: 100%|██████████| 11/11 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 109 images for Cartoon_Stylized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train:  49%|████▊     | 37/76 [00:29<00:25,  1.52it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (93651448 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Cartoon_Stylized → train:  95%|█████████▍| 72/76 [01:05<00:02,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Cartoon Stylized/Objects/cartoon_obj_24.png: Image size (488023972 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train: 100%|██████████| 76/76 [01:10<00:00,  1.08it/s]\n",
            "Cartoon_Stylized → val: 100%|██████████| 22/22 [00:12<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Cartoon Stylized/Objects/cartoon_obj_22.png: Image size (277788889 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → test: 100%|██████████| 11/11 [00:05<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 0 images for .ipynb_checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".ipynb_checkpoints → train: 0it [00:00, ?it/s]\n",
            ".ipynb_checkpoints → val: 0it [00:00, ?it/s]\n",
            ".ipynb_checkpoints → test: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Vector_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art → train: 100%|██████████| 70/70 [00:26<00:00,  2.61it/s]\n",
            "Vector_Art → val: 100%|██████████| 19/19 [00:04<00:00,  4.33it/s]\n",
            "Vector_Art → test: 100%|██████████| 11/11 [00:02<00:00,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset prepared successfully in YOLO classification format!\n",
            "📁 Output location: /content/asset_styles_yolo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "\n",
        "# --- CONFIG ---\n",
        "zip_path = \"/content/Styles.zip\"\n",
        "unzip_path = \"/content/Styles_unzipped\"\n",
        "output_base = \"/content/asset_styles_yolo\"\n",
        "splits = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "augment = True\n",
        "max_dim = 2048\n",
        "pad_color = (114, 114, 114)\n",
        "\n",
        "# --- Step 0: Unzip dataset ---\n",
        "os.makedirs(unzip_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n",
        "\n",
        "# Fix nested parent folder issue (Mac zip often adds extra folder)\n",
        "# Inspect folder structure and pick the first subfolder that isn't __MACOSX\n",
        "subfolders = [f for f in os.listdir(unzip_path)\n",
        "              if os.path.isdir(os.path.join(unzip_path, f)) and not f.startswith(\"__MACOSX\")]\n",
        "if len(subfolders) == 1:\n",
        "    source_dir = os.path.join(unzip_path, subfolders[0])\n",
        "else:\n",
        "    source_dir = unzip_path\n",
        "\n",
        "# --- Step 1: Create base split folders ---\n",
        "for split in splits.keys():\n",
        "    os.makedirs(os.path.join(output_base, split), exist_ok=True)\n",
        "\n",
        "# --- Padding function ---\n",
        "def pad_to_square(img, pad_color=(114, 114, 114)):\n",
        "    w, h = img.size\n",
        "    size = max(w, h)\n",
        "    new_img = Image.new(\"RGB\", (size, size), pad_color)\n",
        "    new_img.paste(img, ((size - w) // 2, (size - h) // 2))\n",
        "    return new_img\n",
        "\n",
        "# --- Step 2: Loop through top-level style folders ---\n",
        "style_folders = [f for f in os.listdir(source_dir)\n",
        "                 if os.path.isdir(os.path.join(source_dir, f)) and not f.startswith(\"__MACOSX\")]\n",
        "\n",
        "# Keep track of classes for data.yaml\n",
        "all_classes = []\n",
        "\n",
        "for style in style_folders:\n",
        "    style_path = os.path.join(source_dir, style)\n",
        "    label_name = style.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "    # Get subfolders (Backgrounds, UI, Characters, Objects)\n",
        "    subfolders = [f for f in os.listdir(style_path)\n",
        "                  if os.path.isdir(os.path.join(style_path, f)) and not f.startswith(\"__MACOSX\")]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        subfolder_path = os.path.join(style_path, subfolder)\n",
        "        subfolder_name = subfolder.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "        # Flatten class name\n",
        "        class_name = f\"{label_name}_{subfolder_name}\"\n",
        "        all_classes.append(class_name)\n",
        "\n",
        "        # Create train/val/test folders\n",
        "        for split in splits.keys():\n",
        "            os.makedirs(os.path.join(output_base, split, class_name), exist_ok=True)\n",
        "\n",
        "        # Gather all image paths in subfolder\n",
        "        image_paths = [os.path.join(subfolder_path, f)\n",
        "                       for f in os.listdir(subfolder_path)\n",
        "                       if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")) and not f.startswith(\".\")]\n",
        "\n",
        "        print(f\"📂 Found {len(image_paths)} images for {class_name}\")\n",
        "\n",
        "        # Shuffle and split\n",
        "        random.shuffle(image_paths)\n",
        "        n = len(image_paths)\n",
        "        train_end = int(splits[\"train\"] * n)\n",
        "        val_end = int((splits[\"train\"] + splits[\"val\"]) * n)\n",
        "\n",
        "        split_data = {\n",
        "            \"train\": image_paths[:train_end],\n",
        "            \"val\": image_paths[train_end:val_end],\n",
        "            \"test\": image_paths[val_end:]\n",
        "        }\n",
        "\n",
        "        # --- Step 3: Process images ---\n",
        "        for split, imgs in split_data.items():\n",
        "            dest_dir = os.path.join(output_base, split, class_name)\n",
        "            for img_path in tqdm(imgs, desc=f\"{class_name} → {split}\"):\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        # Handle transparency\n",
        "                        if img.mode in (\"RGBA\", \"LA\") or (img.mode == \"P\" and \"transparency\" in img.info):\n",
        "                            bg = Image.new(\"RGB\", img.size, pad_color)\n",
        "                            bg.paste(img, mask=img.split()[-1])\n",
        "                            img = bg\n",
        "                        elif img.mode != \"RGB\":\n",
        "                            img = img.convert(\"RGB\")\n",
        "\n",
        "                        # Resize large images\n",
        "                        if max(img.size) > max_dim:\n",
        "                            img.thumbnail((max_dim, max_dim), Image.LANCZOS)\n",
        "\n",
        "                        # Pad to square\n",
        "                        img = pad_to_square(img, pad_color)\n",
        "\n",
        "                        # Save base image\n",
        "                        base_name = os.path.basename(img_path).replace(\" \", \"_\")\n",
        "                        save_path = os.path.join(dest_dir, base_name)\n",
        "                        img.save(save_path, quality=95)\n",
        "\n",
        "                        # Augmentation (train only)\n",
        "                        if augment and split == \"train\":\n",
        "                            # Horizontal Flip\n",
        "                            flipped = ImageOps.mirror(img)\n",
        "                            flipped.save(os.path.join(dest_dir, f\"flip_{base_name}\"), quality=95)\n",
        "\n",
        "                            # Vertical Flip\n",
        "                            flipped_v = ImageOps.flip(img)\n",
        "                            flipped_v.save(os.path.join(dest_dir, f\"vflip_{base_name}\"), quality=95)\n",
        "\n",
        "                            # Small Rotation\n",
        "                            angle = random.uniform(-15, 15)\n",
        "                            rotated = img.rotate(angle, expand=False, resample=Image.BICUBIC)\n",
        "                            rotated.save(os.path.join(dest_dir, f\"rot{int(angle)}_{base_name}\"), quality=95)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "# --- Step 4: Create YOLO data.yaml ---\n",
        "all_classes = sorted(list(set(all_classes)))\n",
        "data_yaml = {\n",
        "    'train': os.path.join(output_base, 'train'),\n",
        "    'val': os.path.join(output_base, 'val'),\n",
        "    'test': os.path.join(output_base, 'test'),\n",
        "    'names': {i: name for i, name in enumerate(all_classes)}\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_base, \"data.yaml\"), 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(\"\\n✅ Dataset prepared successfully!\")\n",
        "print(f\"📁 Output location: {output_base}\")\n",
        "print(f\"📄 YOLO data.yaml created with {len(all_classes)} classes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b626xsLntC-J",
        "outputId": "a286c24c-4b8a-4857-9cbc-8b9d56ead829"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Cartoon_Stylized_UI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized_UI → train: 100%|██████████| 7/7 [00:09<00:00,  1.39s/it]\n",
            "Cartoon_Stylized_UI → val: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
            "Cartoon_Stylized_UI → test: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Cartoon_Stylized_Objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized_Objects → train: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it]\n",
            "Cartoon_Stylized_Objects → val: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "Cartoon_Stylized_Objects → test: 100%|██████████| 1/1 [00:00<00:00, 102.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Cartoon_Stylized_Background\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized_Background → train: 100%|██████████| 7/7 [00:10<00:00,  1.44s/it]\n",
            "Cartoon_Stylized_Background → val: 100%|██████████| 2/2 [00:00<00:00,  2.97it/s]\n",
            "Cartoon_Stylized_Background → test: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Cartoon_Stylized_Characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized_Characters → train:  71%|███████▏  | 5/7 [00:05<00:02,  1.13s/it]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Cartoon Stylized/Characters/cartoon_char_01.png: bad transparency mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized_Characters → train: 100%|██████████| 7/7 [00:06<00:00,  1.00it/s]\n",
            "Cartoon_Stylized_Characters → val: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
            "Cartoon_Stylized_Characters → test: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Vector_Art_UI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_UI → train: 100%|██████████| 7/7 [00:02<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/UI/vector_ui_03.png: bad transparency mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_UI → val: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
            "Vector_Art_UI → test: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Vector_Art_Objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_Objects → train:  71%|███████▏  | 5/7 [00:00<00:00, 29.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/Objects/vector_obj_05.png: bad transparency mask\n",
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/Objects/vector_obj_06.png: bad transparency mask\n",
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/Objects/vector_obj_02.png: bad transparency mask\n",
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/Objects/vector_obj_01.png: bad transparency mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_Objects → train: 100%|██████████| 7/7 [00:01<00:00,  4.64it/s]\n",
            "Vector_Art_Objects → val: 100%|██████████| 2/2 [00:00<00:00,  6.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Vector Art/Objects/vector_obj_04.png: bad transparency mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_Objects → test: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Vector_Art_Background\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_Background → train: 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]\n",
            "Vector_Art_Background → val: 100%|██████████| 2/2 [00:00<00:00,  4.52it/s]\n",
            "Vector_Art_Background → test: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Vector_Art_Characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art_Characters → train: 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]\n",
            "Vector_Art_Characters → val: 100%|██████████| 2/2 [00:00<00:00,  4.65it/s]\n",
            "Vector_Art_Characters → test: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Pixel_Art_UI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art_UI → train: 100%|██████████| 7/7 [00:00<00:00,  9.14it/s]\n",
            "Pixel_Art_UI → val: 100%|██████████| 2/2 [00:00<00:00, 25.17it/s]\n",
            "Pixel_Art_UI → test: 100%|██████████| 1/1 [00:00<00:00, 65.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Pixel_Art_Objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art_Objects → train: 100%|██████████| 7/7 [00:02<00:00,  2.82it/s]\n",
            "Pixel_Art_Objects → val: 100%|██████████| 2/2 [00:00<00:00, 11.16it/s]\n",
            "Pixel_Art_Objects → test: 100%|██████████| 1/1 [00:00<00:00, 41.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Pixel_Art_Background\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art_Background → train: 100%|██████████| 7/7 [00:00<00:00, 19.96it/s]\n",
            "Pixel_Art_Background → val: 100%|██████████| 2/2 [00:00<00:00,  5.42it/s]\n",
            "Pixel_Art_Background → test: 100%|██████████| 1/1 [00:00<00:00, 69.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Pixel_Art_Characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art_Characters → train: 100%|██████████| 7/7 [00:02<00:00,  3.46it/s]\n",
            "Pixel_Art_Characters → val: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
            "Pixel_Art_Characters → test: 100%|██████████| 1/1 [00:00<00:00, 38.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Hand_Painted_UI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted_UI → train: 100%|██████████| 7/7 [00:09<00:00,  1.35s/it]\n",
            "Hand_Painted_UI → val: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n",
            "Hand_Painted_UI → test: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Hand_Painted_Objects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted_Objects → train: 100%|██████████| 7/7 [00:10<00:00,  1.48s/it]\n",
            "Hand_Painted_Objects → val: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
            "Hand_Painted_Objects → test: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Hand_Painted_Background\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted_Background → train: 100%|██████████| 7/7 [00:26<00:00,  3.76s/it]\n",
            "Hand_Painted_Background → val: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n",
            "Hand_Painted_Background → test: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 10 images for Hand_Painted_Characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted_Characters → train: 100%|██████████| 7/7 [00:23<00:00,  3.30s/it]\n",
            "Hand_Painted_Characters → val: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
            "Hand_Painted_Characters → test: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset prepared successfully!\n",
            "📁 Output location: /content/asset_styles_yolo\n",
            "📄 YOLO data.yaml created with 16 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p6VEl4jjSOSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U ultralytics\n"
      ],
      "metadata": {
        "id": "anGdnliHWNM6",
        "outputId": "872cb60b-f3fd-4fc8-9668-761e61e07cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.222)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "CX_xD9nmiyWb",
        "outputId": "493de3f7-23d0-4f22-be20-1741966ab775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 50.8/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11n-cls.pt\")  # pretrained classification model\n",
        "results = model.train(\n",
        "    data=\"/content/asset_styles_yolo\",\n",
        "    epochs=50,\n",
        "    imgsz=224,\n",
        "    batch=8,\n",
        "    lr0=0.001,\n",
        "    weight_decay=0.0005,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NxDoh9Fu3d5",
        "outputId": "b64b4c20-954c-4cee-8e79-752e2538237b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/asset_styles_yolo, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/asset_styles_yolo/train... found 424 images in 16 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/asset_styles_yolo/val... found 31 images in 16 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/asset_styles_yolo/test... found 16 images in 16 classes ✅ \n",
            "Overriding model.yaml nc=80 with nc=16\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    350736  ultralytics.nn.modules.head.Classify         [256, 16]                     \n",
            "YOLO11n-cls summary: 86 layers, 1,551,600 parameters, 1,551,600 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 116.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 155.8±274.0 MB/s, size: 249.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/asset_styles_yolo/train... 424 images, 0 corrupt: 100% ━━━━━━━━━━━━ 424/424 147.8it/s 2.9s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/asset_styles_yolo/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.2±0.3 ms, read: 64.4±30.3 MB/s, size: 474.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/asset_styles_yolo/val... 31 images, 0 corrupt: 100% ━━━━━━━━━━━━ 31/31 227.0it/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/asset_styles_yolo/val.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/classify/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 20.7MB/s 0.0s\n",
            "\u001b[K       1/50     0.195G      2.839          8        224: 100% ━━━━━━━━━━━━ 53/53 1.6it/s 33.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 3.7it/s 0.5s\n",
            "                   all      0.129      0.484\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       2/50     0.195G      2.213          8        224: 100% ━━━━━━━━━━━━ 53/53 2.5it/s 21.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 28.6it/s 0.1s\n",
            "                   all      0.484      0.742\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       3/50     0.195G      1.579          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 28.4it/s 0.1s\n",
            "                   all      0.548      0.742\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       4/50     0.195G       1.06          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 34.9it/s 0.1s\n",
            "                   all      0.581      0.839\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       5/50     0.195G     0.7872          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.3s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 32.6it/s 0.1s\n",
            "                   all      0.516      0.839\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       6/50     0.195G     0.6385          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 18.0it/s 0.1s\n",
            "                   all      0.581      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       7/50     0.195G     0.4847          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 24.1it/s 0.1s\n",
            "                   all      0.548      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       8/50     0.195G     0.4075          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 30.4it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       9/50     0.195G     0.3045          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 29.7it/s 0.1s\n",
            "                   all      0.581      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      10/50     0.195G     0.2781          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 23.7it/s 0.1s\n",
            "                   all      0.581      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      11/50     0.195G     0.2763          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 20.1it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      12/50     0.195G     0.2385          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.3s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 28.8it/s 0.1s\n",
            "                   all      0.613      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      13/50     0.195G     0.2115          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 33.1it/s 0.1s\n",
            "                   all      0.452      0.742\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      14/50     0.195G     0.1851          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 27.2it/s 0.1s\n",
            "                   all      0.548      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      15/50     0.195G     0.1675          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 19.3it/s 0.1s\n",
            "                   all      0.484      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      16/50     0.195G     0.1475          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 34.5it/s 0.1s\n",
            "                   all      0.548      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      17/50     0.195G     0.1924          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 22.6it/s 0.1s\n",
            "                   all      0.581      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      18/50     0.195G     0.1754          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 31.4it/s 0.1s\n",
            "                   all      0.581      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      19/50     0.195G     0.1095          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 18.8it/s 0.1s\n",
            "                   all      0.516      0.839\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      20/50     0.195G     0.1187          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 17.7it/s 0.1s\n",
            "                   all      0.548      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      21/50     0.195G     0.1455          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 18.8it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      22/50     0.195G     0.1177          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 27.4it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      23/50     0.195G     0.1052          8        224: 100% ━━━━━━━━━━━━ 53/53 2.1it/s 24.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 28.0it/s 0.1s\n",
            "                   all      0.548      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      24/50     0.195G     0.1241          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 19.4it/s 0.1s\n",
            "                   all      0.581      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      25/50     0.195G     0.1431          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 31.5it/s 0.1s\n",
            "                   all      0.548      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      26/50     0.195G     0.1264          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 33.9it/s 0.1s\n",
            "                   all      0.516      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      27/50     0.195G     0.1065          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 21.4it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      28/50     0.195G    0.08068          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 20.2it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      29/50     0.195G     0.1148          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 29.1it/s 0.1s\n",
            "                   all      0.484      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      30/50     0.195G    0.09911          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 20.9it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      31/50     0.195G    0.07317          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 22.8it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      32/50     0.195G    0.07133          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 20.6it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      33/50     0.195G    0.08054          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 20.4it/s 0.1s\n",
            "                   all      0.581      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      34/50     0.195G     0.1043          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 32.3it/s 0.1s\n",
            "                   all      0.484      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      35/50     0.195G    0.08069          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 23.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 26.0it/s 0.1s\n",
            "                   all      0.516      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      36/50     0.195G    0.06204          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 16.1it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      37/50     0.195G    0.07461          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 24.3it/s 0.1s\n",
            "                   all      0.452      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      38/50     0.195G    0.05118          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.3s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 27.6it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      39/50     0.195G    0.07801          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 24.9it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      40/50     0.195G    0.07292          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 29.2it/s 0.1s\n",
            "                   all      0.516      0.774\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      41/50     0.195G    0.06239          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 24.9it/s 0.1s\n",
            "                   all      0.516      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      42/50     0.195G    0.06179          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 17.7it/s 0.1s\n",
            "                   all      0.452      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      43/50     0.195G    0.08486          8        224: 100% ━━━━━━━━━━━━ 53/53 2.3it/s 22.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 22.3it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      44/50     0.195G    0.05842          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 26.5it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      45/50     0.195G     0.0531          8        224: 100% ━━━━━━━━━━━━ 53/53 2.4it/s 22.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 37.4it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      46/50     0.195G    0.06611          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 35.4it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      47/50     0.195G    0.08874          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 25.5it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      48/50     0.195G    0.05854          8        224: 100% ━━━━━━━━━━━━ 53/53 2.5it/s 21.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 28.5it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      49/50     0.195G    0.04949          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 24.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 22.1it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      50/50     0.195G    0.05268          8        224: 100% ━━━━━━━━━━━━ 53/53 2.2it/s 23.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 24.9it/s 0.1s\n",
            "                   all      0.484      0.806\n",
            "\n",
            "50 epochs completed in 0.335 hours.\n",
            "Optimizer stripped from /content/runs/classify/train/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from /content/runs/classify/train/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating /content/runs/classify/train/weights/best.pt...\n",
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,546,520 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/asset_styles_yolo/train... found 424 images in 16 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/asset_styles_yolo/val... found 31 images in 16 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/asset_styles_yolo/test... found 16 images in 16 classes ✅ \n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 17.7it/s 0.1s\n",
            "                   all      0.581      0.839\n",
            "Speed: 0.1ms preprocess, 2.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/classify/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Path to your validation dataset\n",
        "val_dir = \"/content/asset_styles_yolo/val\"\n",
        "\n",
        "# List your class names (must match folder names)\n",
        "class_names = [\"Hand_Painted_Background\", \"Hand_Painted_Characters\",\"Hand_Painted_Objects\",\"Hand_Painted_UI\",\n",
        "               \"Cartoon_Stylized_Background\", \"Cartoon_Stylized_Characters\", \"Cartoon_Stylized_Objects\", \"Cartoon_Stylized_UI\",\n",
        "               \"Pixel_Art_Background\", \"Pixel_Art_Characters\", \"Pixel_Art_Objects\", \"Pixel_Art_UI\",\n",
        "               \"Vector_Art_Background\", \"Vector_Art_Characters\", \"Vector_Art_Objects\", \"Vector_Art_UI\"]\n",
        "\n",
        "# Load your trained model (trained with nc=5)\n",
        "model = YOLO(\"/content/yolo11n-cls.pt\")\n",
        "\n",
        "# Prepare lists to store true and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Iterate through each class folder\n",
        "for label_idx, class_name in enumerate(class_names):\n",
        "    class_folder = Path(val_dir) / class_name\n",
        "    if not class_folder.exists():\n",
        "        print(f\"Warning: Folder {class_folder} does not exist.\")\n",
        "        continue\n",
        "\n",
        "    # Iterate through each image in the folder\n",
        "    for img_path in class_folder.iterdir():\n",
        "        if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"]:\n",
        "            continue  # skip unsupported files\n",
        "\n",
        "        # Run prediction\n",
        "        results = model.predict(source=str(img_path), save=False, verbose=False)\n",
        "        pred_class = results[0].probs.top1  # predicted class index\n",
        "\n",
        "        # Store true and predicted labels\n",
        "        y_true.append(label_idx)\n",
        "        y_pred.append(pred_class)\n",
        "\n",
        "# --- Fix for extra class in the model (ignore predictions of class index 4) ---\n",
        "y_true_filtered = []\n",
        "y_pred_filtered = []\n",
        "\n",
        "for t, p in zip(y_true, y_pred):\n",
        "    if p < len(class_names):  # keep only predictions for valid classes (0-3)\n",
        "        y_true_filtered.append(t)\n",
        "        y_pred_filtered.append(p)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0)\n",
        "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=list(range(len(class_names))))\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Compute additional metrics\n",
        "accuracy_top1 = accuracy_score(y_true_filtered, y_pred_filtered)\n",
        "\n",
        "# Print results\n",
        "print(\"Top-1 Accuracy:\", accuracy_top1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "660BB9J0Qren",
        "outputId": "2202837f-f95b-4ed4-beb0-b18001901ff8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: nan\n",
            "Precision: nan\n",
            "Recall: nan\n",
            "F1 Score: nan\n",
            "Confusion Matrix:\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:557: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# --- Assume y_true_filtered and y_pred_filtered are already computed ---\n",
        "# Example:\n",
        "# y_true_filtered = [...]\n",
        "# y_pred_filtered = [...]\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=list(range(len(class_names))))\n",
        "\n",
        "# Create a figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot using seaborn heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Save to file\n",
        "save_path = '/content/runs/classify/val/confusion_matrix.png'\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✅ Confusion matrix saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "cUzhow3ZMWqd",
        "outputId": "a35efd36-75f0-4d7f-faef-90eddab6ad9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/runs/classify/val/confusion_matrix.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-595035119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Save to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/runs/classify/val/confusion_matrix.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/runs/classify/val/confusion_matrix.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(source=\"/content/asset_styles_yolo/test\", imgsz=224)\n",
        "for r in results:\n",
        "    class_idx = int(r.probs.top1)\n",
        "    class_name = r.names[class_idx]\n",
        "    confidence = float(r.probs[class_idx])\n",
        "    style, asset_type = class_name.split(\"_\")\n",
        "    print(f\"{r.path} → Style: {style}, Asset: {asset_type}, Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "iUvlkDqbfylP",
        "outputId": "7676efef-f672-4e99-8b96-77aa318efa05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No images or videos found in /content/asset_styles_yolo/test. Supported formats are:\nimages: {'bmp', 'jpeg', 'heic', 'tiff', 'jpg', 'pfm', 'mpo', 'webp', 'png', 'dng', 'tif'}\nvideos: {'mov', 'm4v', 'webm', 'wmv', 'mpg', 'asf', 'mp4', 'avi', 'mkv', 'ts', 'mpeg', 'gif'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2196014742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/asset_styles_yolo/test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     def track(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for thread-safe inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# Setup source every time predict is called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# Check if save_dir/ label file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/classify/predict.py\u001b[0m in \u001b[0;36msetup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;34m\"\"\"Set up source and inference mode and classify transforms.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         updated = (\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36msetup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_imgsz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# check image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         self.dataset = load_inference_source(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36mload_inference_source\u001b[0;34m(source, batch, vid_stride, buffer, channels)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadPilAndNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadImagesAndVideos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_stride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Attach source types to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/loaders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, batch, vid_stride, channels)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No images or videos found in {p}. {FORMATS_HELP_MSG}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No images or videos found in /content/asset_styles_yolo/test. Supported formats are:\nimages: {'bmp', 'jpeg', 'heic', 'tiff', 'jpg', 'pfm', 'mpo', 'webp', 'png', 'dng', 'tif'}\nvideos: {'mov', 'm4v', 'webm', 'wmv', 'mpg', 'asf', 'mp4', 'avi', 'mkv', 'ts', 'mpeg', 'gif'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have already computed:\n",
        "# metrics.top1, metrics.top5, and filtered precision, recall, f1\n",
        "metrics_dict = {\n",
        "    \"Top-1 Accuracy\": metrics.top1,\n",
        "    \"Top-5 Accuracy\": metrics.top5,\n",
        "    \"Precision\": precision,  # from your filtered evaluation\n",
        "    \"Recall\": recall,        # from your filtered evaluation\n",
        "    \"F1 Score\": f1           # from your filtered evaluation\n",
        "}\n",
        "\n",
        "# Save to CSV\n",
        "save_path = \"/content/classification_metrics.csv\"\n",
        "pd.DataFrame([metrics_dict]).to_csv(save_path, index=False)\n",
        "print(f\"✅ Metrics saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "dYUJqzkTMYdU",
        "outputId": "b8f04ebd-3ca9-4f66-d010-9c6902eeabfc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3981749052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# metrics.top1, metrics.top5, and filtered precision, recall, f1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m metrics_dict = {\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"Top-1 Accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"Top-5 Accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"Precision\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# from your filtered evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "# Load your trained model (use the best weights)\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Path to your test image\n",
        "test_image = \"/content/Styles_unzipped/Pixel Art/Background/pixel_bg_05.png\"  # change this to any image path\n",
        "\n",
        "# Run prediction\n",
        "results = model.predict(source=test_image, show=True)\n",
        "\n",
        "# Print predicted class\n",
        "print(\"Predicted class:\", results[0].names[results[0].probs.top1])\n",
        "print(\"Confidence:\", results[0].probs.top1conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKA26iWlNLLf",
        "outputId": "a5617818-0c85-4e31-de8a-1c67f369044d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "\n",
            "image 1/1 /content/Styles_unzipped/Pixel Art/Background/pixel_bg_05.png: 224x224 IAT_360_CV_Project_Data 1.00, 5.6ms\n",
            "Speed: 3.2ms preprocess, 5.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Predicted class: IAT_360_CV_Project_Data\n",
            "Confidence: tensor(1., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "# Load trained model\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Path to test image\n",
        "test_image = \"/content/Styles_unzipped/Pixel Art/Background/pixel_bg_05.png\"\n",
        "\n",
        "# Run prediction\n",
        "results = model.predict(source=test_image, verbose=False)\n",
        "\n",
        "# Get top-1 predicted index and confidence\n",
        "top1_idx = results[0].probs.top1\n",
        "top1_conf = results[0].probs.top1conf\n",
        "\n",
        "# Map index to class name\n",
        "# Filter out the 5th class if it exists\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]\n",
        "if top1_idx >= len(class_names):\n",
        "    predicted_class = \"Unknown / Extra Class\"\n",
        "else:\n",
        "    predicted_class = class_names[top1_idx]\n",
        "\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Confidence: {top1_conf:.4f}\")"
      ],
      "metadata": {
        "id": "IacDaTFmWkyx",
        "outputId": "e9be73b9-535b-4cb1-c38e-554cf2d88d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Hand_Painted\n",
            "Confidence: 1.0000\n"
          ]
        }
      ]
    }
  ]
}