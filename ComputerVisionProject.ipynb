{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BCBLWEHPxeW"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zm-f21/My-Computer-Vision-Project/blob/Branch-3/ComputerVisionProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b> Object Detection Using YOLO <b> </h1></center>"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is designed to provide a comprehensive understanding of how to use YOLO, a state-of-the-art method in computer vision, for detecting objects in images.\n",
        "\n",
        "Object detection and classification is a key technology in many areas, such as automated vehicles, security, and even healthcare.\n",
        "\n",
        "We will begin with the basics of preparing (pre processing) an image dataset, ensuring it is ready for effective model training.We will then explore how YOLO, a type of convolutional neural network, automatically extracts features from images to recognize different objects. Understanding this process is crucial for grasping how YOLO operates.\n",
        "\n",
        "The core of this tutorial is focused on transfer learning using YOLO. We will teach you how to take a pre-trained YOLO model and adapt it to a new dataset. This technique is efficient and powerful, allowing us to harness the strengths of YOLO with less computational effort.\n",
        "\n",
        "By the end of this tutorial, you will have hands-on experience with preparing data, implementing YOLO, and understanding the principles behind it. This tutorial aims to equip students with practical skills and knowledge in one of the most exciting fields in technology."
      ],
      "metadata": {
        "id": "ZIi4bjh7mIJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since its inception, the YOLO family of object detection models has come a long way. YOLOv8 is the most recent addition to this famous anchor-based single-shot family of object detectors. It comes with a bunch of improvements which include state-of-the-art accuracy and speed.  In this article, we will be fine tuning the YOLOv8 object detection model on a real-world pothole detection dataset."
      ],
      "metadata": {
        "id": "vsmlgXapu5rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous lecture, you were asked to make your own custom datasets for a project you want to work on. Today we will explore how to finetune YOLO on a certain dataset.  "
      ],
      "metadata": {
        "id": "v5zL0avFnNaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "rr-j-jCmaHId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources:\n",
        "# Yolo Classification Model: https://docs.ultralytics.com/tasks/classify/#val"
      ],
      "metadata": {
        "id": "-4UGKW3X7ja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run the code below when you want to clear files:\n",
        "# !rm -rf /content/*"
      ],
      "metadata": {
        "id": "GNNIiYig7l6t"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create a folder for the unzipped files\n",
        "os.makedirs(\"/content/Styles_unzipped\", exist_ok=True)\n",
        "\n",
        "# Unzip the file into that folder\n",
        "with zipfile.ZipFile(\"/content/Styles.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/Styles_unzipped\")\n",
        "\n",
        "# Verify contents\n",
        "os.listdir(\"/content/Styles_unzipped\")\n"
      ],
      "metadata": {
        "id": "Qeb4_GgaaiRZ",
        "outputId": "53659afb-980f-47db-c1ba-7cd377b71b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Styles']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import shutil\n",
        "import os, random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# #/\n",
        "# base_dir = \"/content/asset_styles\"\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Paths for train/val/test\n",
        "# for split in [\"train\", \"val\", \"test\"]:\n",
        "#     os.makedirs(os.path.join(base_dir, split), exist_ok=True)\n",
        "#     /#\n"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# OLD CODE OCT 26:\n",
        "# The Code Below is the same as the Code Above but it resizes large images to\n",
        "# avoid bomb decrompression and to convert tranparent images into RGB\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "source_dir = \"/content/Styles_unzipped\"   # parent folder\n",
        "output_base = \"/content/asset_styles_yolo\"       # YOLO-ready dataset output\n",
        "splits = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "augment = True  # enable data augmentation\n",
        "\n",
        "# --- Step 1: Create split folders ---\n",
        "for split in splits.keys():\n",
        "    os.makedirs(os.path.join(output_base, split), exist_ok=True)\n",
        "\n",
        "# --- Step 2: Loop through style folders (Cartoon Stylized, Pixel Art, Vector Art) ---\n",
        "style_folders = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]\n",
        "\n",
        "for style in style_folders:\n",
        "    style_path = os.path.join(source_dir, style)\n",
        "    label_name = style.replace(\" \", \"_\")  # YOLO doesn’t like spaces in class names\n",
        "\n",
        "    # Create subfolders for each split\n",
        "    for split in splits.keys():\n",
        "        os.makedirs(os.path.join(output_base, split, label_name), exist_ok=True)\n",
        "\n",
        "    # Collect all image paths from all subfolders (Background, Characters, Object, UI, etc.)\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(style_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                image_paths.append(os.path.join(root, f))\n",
        "\n",
        "    print(f\"📂 Found {len(image_paths)} images for {label_name}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(image_paths)\n",
        "    n = len(image_paths)\n",
        "    train_end = int(splits[\"train\"] * n)\n",
        "    val_end = int((splits[\"train\"] + splits[\"val\"]) * n)\n",
        "\n",
        "    split_data = {\n",
        "        \"train\": image_paths[:train_end],\n",
        "        \"val\": image_paths[train_end:val_end],\n",
        "        \"test\": image_paths[val_end:]\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Copy and augment images ---\n",
        "    for split, imgs in split_data.items():\n",
        "        dest_dir = os.path.join(output_base, split, label_name)\n",
        "        for img_path in tqdm(imgs, desc=f\"{label_name} → {split}\"):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    # --- Fix transparency / color mode ---\n",
        "                    if img.mode == \"RGBA\":\n",
        "                        img = img.convert(\"RGB\")\n",
        "\n",
        "                    # --- Resize if image is very large ---\n",
        "                    if max(img.size) > 2048:\n",
        "                        img.thumbnail((2048, 2048))  # keeps aspect ratio\n",
        "\n",
        "                    base_name = os.path.basename(img_path)\n",
        "                    save_path = os.path.join(dest_dir, base_name)\n",
        "                    img.save(save_path)\n",
        "\n",
        "                    # --- Step 4: Data Augmentation (optional) ---\n",
        "                    if augment and split == \"train\":\n",
        "                        # Horizontal Flip\n",
        "                        flipped = ImageOps.mirror(img)\n",
        "                        flipped.save(os.path.join(dest_dir, \"flip_\" + base_name))\n",
        "\n",
        "                        # Vertical Flip\n",
        "                        flipped_v = ImageOps.flip(img)\n",
        "                        flipped_v.save(os.path.join(dest_dir, \"vflip_\" + base_name))\n",
        "\n",
        "                        # Small Rotation (-15 to +15 degrees)\n",
        "                        angle = random.uniform(-15, 15)\n",
        "                        rotated = img.rotate(angle, expand=True)\n",
        "                        rotated.save(os.path.join(dest_dir, f\"rot{int(angle)}_\" + base_name))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Dataset prepared successfully in YOLO classification format!\")\n",
        "print(f\"📁 Output location: {output_base}\")\n"
      ],
      "metadata": {
        "id": "8zZfEJ-GlKJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fb8479-e803-4f97-a7f4-9c2db76e4e5f",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Hand_Painted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted → train: 100%|██████████| 70/70 [01:51<00:00,  1.60s/it]\n",
            "Hand_Painted → val: 100%|██████████| 19/19 [00:10<00:00,  1.76it/s]\n",
            "Hand_Painted → test: 100%|██████████| 11/11 [00:03<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Pixel_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art → train: 100%|██████████| 70/70 [00:08<00:00,  7.92it/s]\n",
            "Pixel_Art → val: 100%|██████████| 19/19 [00:00<00:00, 33.05it/s]\n",
            "Pixel_Art → test: 100%|██████████| 11/11 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 109 images for Cartoon_Stylized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train:  49%|████▊     | 37/76 [00:29<00:25,  1.52it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (93651448 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Cartoon_Stylized → train:  95%|█████████▍| 72/76 [01:05<00:02,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Cartoon Stylized/Objects/cartoon_obj_24.png: Image size (488023972 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train: 100%|██████████| 76/76 [01:10<00:00,  1.08it/s]\n",
            "Cartoon_Stylized → val: 100%|██████████| 22/22 [00:12<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Cartoon Stylized/Objects/cartoon_obj_22.png: Image size (277788889 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → test: 100%|██████████| 11/11 [00:05<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 0 images for .ipynb_checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".ipynb_checkpoints → train: 0it [00:00, ?it/s]\n",
            ".ipynb_checkpoints → val: 0it [00:00, ?it/s]\n",
            ".ipynb_checkpoints → test: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Vector_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art → train: 100%|██████████| 70/70 [00:26<00:00,  2.61it/s]\n",
            "Vector_Art → val: 100%|██████████| 19/19 [00:04<00:00,  4.33it/s]\n",
            "Vector_Art → test: 100%|██████████| 11/11 [00:02<00:00,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset prepared successfully in YOLO classification format!\n",
            "📁 Output location: /content/asset_styles_yolo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/asset_styles_yolo\"\n",
        "\n",
        "# Check if the folder exists before deleting\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"✅ Deleted folder and all contents: {folder_path}\")\n",
        "else:\n",
        "    print(f\"⚠️ Folder not found: {folder_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umkf21I46yA5",
        "outputId": "e078c23f-d48b-4e2c-a783-73121a785810"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Folder not found: /content/asset_styles_yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = None  # avoid decompression bombs\n",
        "\n",
        "# --- CONFIG ---\n",
        "source_dir = \"/content/Styles_unzipped/Styles\"   # folder containing only your class folders\n",
        "output_base = \"/content/asset_styles_yolo\"       # YOLO-ready dataset output\n",
        "splits = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "augment = True\n",
        "max_dim = 2048\n",
        "pad_color = (114, 114, 114)  # YOLO neutral padding\n",
        "\n",
        "# --- Step 1: Gather only valid style folders ---\n",
        "all_folders = [\n",
        "    f for f in os.listdir(source_dir)\n",
        "    if os.path.isdir(os.path.join(source_dir, f)) and not f.startswith(\".\")\n",
        "]\n",
        "style_folders = sorted(all_folders)  # sort to keep consistent class order\n",
        "print(f\"Detected style classes: {style_folders}\")\n",
        "\n",
        "# --- Step 2: Create split folders for each class ---\n",
        "for split in splits.keys():\n",
        "    for style in style_folders:\n",
        "        os.makedirs(os.path.join(output_base, split, style.replace(\" \", \"_\")), exist_ok=True)\n",
        "\n",
        "# --- Helper function: pad to square ---\n",
        "def pad_to_square(img, pad_color=(114, 114, 114)):\n",
        "    w, h = img.size\n",
        "    size = max(w, h)\n",
        "    new_img = Image.new(\"RGB\", (size, size), pad_color)\n",
        "    new_img.paste(img, ((size - w) // 2, (size - h) // 2))\n",
        "    return new_img\n",
        "\n",
        "# --- Step 3: Process each style folder ---\n",
        "for style in style_folders:\n",
        "    style_path = os.path.join(source_dir, style)\n",
        "    label_name = style.replace(\" \", \"_\")\n",
        "\n",
        "    # Gather all image paths\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(style_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                image_paths.append(os.path.join(root, f))\n",
        "\n",
        "    print(f\"📂 Found {len(image_paths)} images for {label_name}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(image_paths)\n",
        "    n = len(image_paths)\n",
        "    train_end = int(splits[\"train\"] * n)\n",
        "    val_end = int((splits[\"train\"] + splits[\"val\"]) * n)\n",
        "\n",
        "    split_data = {\n",
        "        \"train\": image_paths[:train_end],\n",
        "        \"val\": image_paths[train_end:val_end],\n",
        "        \"test\": image_paths[val_end:],\n",
        "    }\n",
        "\n",
        "    # --- Step 4: Process and augment images ---\n",
        "    for split, imgs in split_data.items():\n",
        "        dest_dir = os.path.join(output_base, split, label_name)\n",
        "        for img_path in tqdm(imgs, desc=f\"{label_name} → {split}\"):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    # --- Handle transparency ---\n",
        "                    if img.mode in (\"RGBA\", \"LA\"):\n",
        "                        bg = Image.new(\"RGB\", img.size, pad_color)\n",
        "                        bg.paste(img, mask=img.split()[-1])\n",
        "                        img = bg\n",
        "                    elif img.mode == \"P\":\n",
        "                        if \"transparency\" in img.info:\n",
        "                            try:\n",
        "                                alpha = img.convert(\"RGBA\")\n",
        "                                bg = Image.new(\"RGB\", img.size, pad_color)\n",
        "                                bg.paste(alpha, mask=alpha.split()[-1])\n",
        "                                img = bg\n",
        "                            except Exception:\n",
        "                                img = img.convert(\"RGB\")\n",
        "                        else:\n",
        "                            img = img.convert(\"RGB\")\n",
        "                    elif img.mode != \"RGB\":\n",
        "                        img = img.convert(\"RGB\")\n",
        "\n",
        "                    # --- Resize large images ---\n",
        "                    if max(img.size) > max_dim:\n",
        "                        img.thumbnail((max_dim, max_dim), Image.LANCZOS)\n",
        "\n",
        "                    # --- Pad to square ---\n",
        "                    img = pad_to_square(img, pad_color)\n",
        "\n",
        "                    # --- Save cleaned image ---\n",
        "                    base_name = os.path.basename(img_path).replace(\" \", \"_\")\n",
        "                    save_path = os.path.join(dest_dir, base_name)\n",
        "                    img.save(save_path, quality=95)\n",
        "\n",
        "                    # --- Augment training images ---\n",
        "                    if augment and split == \"train\":\n",
        "                        # Horizontal flip\n",
        "                        flipped = ImageOps.mirror(img)\n",
        "                        flipped.save(os.path.join(dest_dir, f\"flip_{base_name}\"), quality=95)\n",
        "\n",
        "                        # Vertical flip\n",
        "                        flipped_v = ImageOps.flip(img)\n",
        "                        flipped_v.save(os.path.join(dest_dir, f\"vflip_{base_name}\"), quality=95)\n",
        "\n",
        "                        # Small rotation\n",
        "                        angle = random.uniform(-15, 15)\n",
        "                        rotated = img.rotate(angle, expand=False, resample=Image.BICUBIC)\n",
        "                        rotated.save(os.path.join(dest_dir, f\"rot{int(angle)}_{base_name}\"), quality=95)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Dataset prepared successfully in YOLO classification format!\")\n",
        "print(f\"📁 Output location: {output_base}\")"
      ],
      "metadata": {
        "id": "Xt0cEkQEMf5g",
        "outputId": "2394bdf3-ec15-4ae3-ec48-e2301f440627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected style classes: ['Cartoon Stylized', 'Hand Painted', 'Pixel Art', 'Vector Art']\n",
            "📂 Found 100 images for Cartoon_Stylized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train:  47%|████▋     | 33/70 [01:13<01:40,  2.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Styles/Cartoon Stylized/Objects/cartoon_obj_24.png: image file is truncated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train: 100%|██████████| 70/70 [02:07<00:00,  1.83s/it]\n",
            "Cartoon_Stylized → val: 100%|██████████| 19/19 [00:12<00:00,  1.51it/s]\n",
            "Cartoon_Stylized → test: 100%|██████████| 11/11 [00:07<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Hand_Painted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted → train: 100%|██████████| 70/70 [02:14<00:00,  1.92s/it]\n",
            "Hand_Painted → val: 100%|██████████| 19/19 [00:09<00:00,  1.96it/s]\n",
            "Hand_Painted → test: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Pixel_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art → train: 100%|██████████| 70/70 [00:16<00:00,  4.20it/s]\n",
            "Pixel_Art → val: 100%|██████████| 19/19 [00:01<00:00, 14.73it/s]\n",
            "Pixel_Art → test: 100%|██████████| 11/11 [00:00<00:00, 32.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Vector_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art → train: 100%|██████████| 70/70 [00:56<00:00,  1.24it/s]\n",
            "Vector_Art → val: 100%|██████████| 19/19 [00:04<00:00,  4.27it/s]\n",
            "Vector_Art → test: 100%|██████████| 11/11 [00:03<00:00,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset prepared successfully in YOLO classification format!\n",
            "📁 Output location: /content/asset_styles_yolo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW CODE: OCT 29 (FIXED FOR BAD TRANSPARENCY MASK)\n",
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "Image.MAX_IMAGE_PIXELS = None #to fix the large images\n",
        "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# --- CONFIG ---\n",
        "source_dir = \"/content/Styles_unzipped/Styles\"         # parent folder\n",
        "output_base = \"/content/asset_styles_yolo\"      # YOLO-ready dataset output\n",
        "splits = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "augment = True  # enable data augmentation\n",
        "max_dim = 2048  # cap to avoid decompression bomb\n",
        "pad_color = (114, 114, 114)  # YOLO-style neutral padding\n",
        "\n",
        "# --- Step 1: Create split folders ---\n",
        "for split in splits.keys():\n",
        "    os.makedirs(os.path.join(output_base, split), exist_ok=True)\n",
        "\n",
        "# --- Step 2: Loop through style folders ---\n",
        "style_folders = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]\n",
        "\n",
        "def pad_to_square(img, pad_color=(114, 114, 114)):\n",
        "    \"\"\"Pads non-square images to square shape without resizing.\"\"\"\n",
        "    w, h = img.size\n",
        "    size = max(w, h)\n",
        "    new_img = Image.new(\"RGB\", (size, size), pad_color)\n",
        "    new_img.paste(img, ((size - w) // 2, (size - h) // 2))\n",
        "    return new_img\n",
        "\n",
        "for style in style_folders:\n",
        "    style_path = os.path.join(source_dir, style)\n",
        "    label_name = style.replace(\" \", \"_\").replace(\"-\", \"_\")  # sanitize names\n",
        "\n",
        "    # Create subfolders for each split\n",
        "    for split in splits.keys():\n",
        "        os.makedirs(os.path.join(output_base, split, label_name), exist_ok=True)\n",
        "\n",
        "    # Gather all image paths\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(style_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                image_paths.append(os.path.join(root, f))\n",
        "\n",
        "    print(f\"📂 Found {len(image_paths)} images for {label_name}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(image_paths)\n",
        "    n = len(image_paths)\n",
        "    train_end = int(splits[\"train\"] * n)\n",
        "    val_end = int((splits[\"train\"] + splits[\"val\"]) * n)\n",
        "\n",
        "    split_data = {\n",
        "        \"train\": image_paths[:train_end],\n",
        "        \"val\": image_paths[train_end:val_end],\n",
        "        \"test\": image_paths[val_end:]\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Process and augment images ---\n",
        "    for split, imgs in split_data.items():\n",
        "        dest_dir = os.path.join(output_base, split, label_name)\n",
        "        for img_path in tqdm(imgs, desc=f\"{label_name} → {split}\"):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    # --- FIXED TRANSPARENCY HANDLING ---\n",
        "                    if img.mode in (\"RGBA\", \"LA\"):\n",
        "                        # Handle true alpha channel\n",
        "                        bg = Image.new(\"RGB\", img.size, pad_color)\n",
        "                        bg.paste(img, mask=img.split()[-1])\n",
        "                        img = bg\n",
        "                    elif img.mode == \"P\":\n",
        "                        # Handle palette-based images safely\n",
        "                        if \"transparency\" in img.info:\n",
        "                            try:\n",
        "                                alpha = img.convert(\"RGBA\")\n",
        "                                bg = Image.new(\"RGB\", img.size, pad_color)\n",
        "                                bg.paste(alpha, mask=alpha.split()[-1])\n",
        "                                img = bg\n",
        "                            except Exception:\n",
        "                                img = img.convert(\"RGB\")  # fallback if transparency fails\n",
        "                        else:\n",
        "                            img = img.convert(\"RGB\")\n",
        "                    elif img.mode != \"RGB\":\n",
        "                        img = img.convert(\"RGB\")\n",
        "\n",
        "                    # --- Resize large images ---\n",
        "                    if max(img.size) > max_dim:\n",
        "                        img.thumbnail((max_dim, max_dim), Image.LANCZOS)\n",
        "\n",
        "                    # --- Pad to square (important for YOLO classification) ---\n",
        "                    img = pad_to_square(img, pad_color)\n",
        "\n",
        "                    # --- Save cleaned base image ---\n",
        "                    base_name = os.path.basename(img_path)\n",
        "                    base_name = base_name.replace(\" \", \"_\")  # clean filename\n",
        "                    save_path = os.path.join(dest_dir, base_name)\n",
        "                    img.save(save_path, quality=95)\n",
        "\n",
        "                    # --- Augmentation (train only) ---\n",
        "                    if augment and split == \"train\":\n",
        "                        # Horizontal Flip\n",
        "                        flipped = ImageOps.mirror(img)\n",
        "                        flipped.save(os.path.join(dest_dir, f\"flip_{base_name}\"), quality=95)\n",
        "\n",
        "                        # Vertical Flip\n",
        "                        flipped_v = ImageOps.flip(img)\n",
        "                        flipped_v.save(os.path.join(dest_dir, f\"vflip_{base_name}\"), quality=95)\n",
        "\n",
        "                        # Small Rotation (within safe bounds)\n",
        "                        angle = random.uniform(-15, 15)\n",
        "                        rotated = img.rotate(angle, expand=False, resample=Image.BICUBIC)\n",
        "                        rotated.save(os.path.join(dest_dir, f\"rot{int(angle)}_{base_name}\"), quality=95)\n",
        "\n",
        "            except Exception as e:\n",
        "                # If the image is corrupted or transparency fails, skip gracefully\n",
        "                print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Dataset prepared successfully in YOLO classification format!\")\n",
        "print(f\"📁 Output location: {output_base}\")"
      ],
      "metadata": {
        "id": "b626xsLntC-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faea6f5d-561b-48d2-eede-59da266e275c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Cartoon_Stylized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train:  16%|█▌        | 11/70 [00:25<02:21,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping /content/Styles_unzipped/Styles/Cartoon Stylized/Objects/cartoon_obj_24.png: image file is truncated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cartoon_Stylized → train: 100%|██████████| 70/70 [01:56<00:00,  1.67s/it]\n",
            "Cartoon_Stylized → val: 100%|██████████| 19/19 [00:12<00:00,  1.50it/s]\n",
            "Cartoon_Stylized → test: 100%|██████████| 11/11 [00:11<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Vector_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vector_Art → train: 100%|██████████| 70/70 [00:56<00:00,  1.25it/s]\n",
            "Vector_Art → val: 100%|██████████| 19/19 [00:06<00:00,  3.13it/s]\n",
            "Vector_Art → test: 100%|██████████| 11/11 [00:03<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Pixel_Art\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pixel_Art → train: 100%|██████████| 70/70 [00:16<00:00,  4.37it/s]\n",
            "Pixel_Art → val: 100%|██████████| 19/19 [00:00<00:00, 23.22it/s]\n",
            "Pixel_Art → test: 100%|██████████| 11/11 [00:01<00:00,  5.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Found 100 images for Hand_Painted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hand_Painted → train: 100%|██████████| 70/70 [02:16<00:00,  1.96s/it]\n",
            "Hand_Painted → val: 100%|██████████| 19/19 [00:09<00:00,  2.00it/s]\n",
            "Hand_Painted → test: 100%|██████████| 11/11 [00:06<00:00,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Dataset prepared successfully in YOLO classification format!\n",
            "📁 Output location: /content/asset_styles_yolo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "CX_xD9nmiyWb",
        "outputId": "ea3b81b9-9eff-4899-a17c-3ec71861a854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 49.4/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sympy==1.12\n",
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vTR2D_S9vtTt",
        "outputId": "70265571-7e09-405f-9ffe-8776c6d1d598"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0 requires sympy>=1.13.3, but you have sympy 1.12 which is incompatible.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "c1c4f9e2ef9b483fbb7ac05fb9dea51a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "64431b1e3b1b48caac07368fa55154a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.222)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 39, in create_package_set_from_installed\n",
            "    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolo11n-cls.pt\")  # pretrained classification model\n",
        "results = model.train(\n",
        "    data=\"/content/asset_styles_yolo\",\n",
        "    epochs=50,\n",
        "    imgsz=224,\n",
        "    batch=8,\n",
        "    lr0=0.001,\n",
        "    weight_decay=0.0005,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NxDoh9Fu3d5",
        "outputId": "d8532ce6-eae6-409d-f246-8ca4a655f20d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt': 100% ━━━━━━━━━━━━ 5.5MB 69.8MB/s 0.1s\n",
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/asset_styles_yolo, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/asset_styles_yolo/train... found 1116 images in 4 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/asset_styles_yolo/val... found 76 images in 4 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/asset_styles_yolo/test... found 44 images in 4 classes ✅ \n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    335364  ultralytics.nn.modules.head.Classify         [256, 4]                      \n",
            "YOLO11n-cls summary: 86 layers, 1,536,228 parameters, 1,536,228 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 69.5MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2234.4±654.6 MB/s, size: 266.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/asset_styles_yolo/train... 1116 images, 0 corrupt: 100% ━━━━━━━━━━━━ 1116/1116 1.5Kit/s 0.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/asset_styles_yolo/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 442.3±283.3 MB/s, size: 252.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/asset_styles_yolo/val... 76 images, 0 corrupt: 100% ━━━━━━━━━━━━ 76/76 679.9it/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/asset_styles_yolo/val.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/classify/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       1/50     0.389G      1.162          4        224: 100% ━━━━━━━━━━━━ 140/140 2.4it/s 58.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 42.6it/s 0.1s\n",
            "                   all      0.513          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       2/50     0.389G     0.7886          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 49.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 25.0it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       3/50     0.389G      0.629          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 17.0it/s 0.3s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       4/50     0.389G     0.6515          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 17.5it/s 0.3s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       5/50     0.389G     0.6413          4        224: 100% ━━━━━━━━━━━━ 140/140 2.9it/s 48.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 20.5it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       6/50     0.389G     0.6021          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 32.8it/s 0.2s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       7/50     0.389G     0.5079          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 30.7it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       8/50     0.393G     0.4657          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.7it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       9/50       0.4G     0.4486          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.6it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      10/50      0.41G     0.4344          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 49.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 29.3it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      11/50     0.418G     0.3819          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.2it/s 0.2s\n",
            "                   all      0.592          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      12/50     0.428G     0.3678          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 49.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.7it/s 0.2s\n",
            "                   all      0.658          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      13/50     0.436G     0.3509          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 26.0it/s 0.2s\n",
            "                   all      0.592          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      14/50     0.443G      0.293          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 33.8it/s 0.1s\n",
            "                   all      0.658          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      15/50     0.453G     0.3359          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.4it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      16/50     0.461G     0.2908          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.4it/s 0.2s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      17/50     0.471G      0.258          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 28.7it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      18/50     0.479G     0.2515          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 19.4it/s 0.3s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      19/50     0.488G     0.2942          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 18.0it/s 0.3s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      20/50     0.496G     0.2118          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 17.0it/s 0.3s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      21/50     0.506G     0.2105          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 30.5it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      22/50     0.514G     0.1963          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.2s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.0it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      23/50     0.523G     0.2141          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 26.2it/s 0.2s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      24/50     0.531G     0.1969          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 32.9it/s 0.2s\n",
            "                   all      0.658          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      25/50     0.541G     0.2058          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 18.3it/s 0.3s\n",
            "                   all      0.645          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      26/50     0.549G     0.1955          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.1s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 22.6it/s 0.2s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      27/50     0.557G     0.1573          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 25.0it/s 0.2s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      28/50     0.566G     0.1455          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.3s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 26.9it/s 0.2s\n",
            "                   all      0.645          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      29/50     0.574G     0.1312          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 25.4it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      30/50     0.584G     0.1327          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.0it/s 0.2s\n",
            "                   all      0.605          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      31/50     0.592G      0.152          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 49.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 18.0it/s 0.3s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      32/50     0.602G    0.09387          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 18.3it/s 0.3s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      33/50     0.609G     0.1077          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 26.5it/s 0.2s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      34/50     0.619G     0.1283          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 29.4it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      35/50     0.627G    0.08624          4        224: 100% ━━━━━━━━━━━━ 140/140 2.6it/s 52.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 22.3it/s 0.2s\n",
            "                   all      0.684          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      36/50     0.637G    0.09015          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 29.1it/s 0.2s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      37/50     0.645G    0.07908          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 27.0it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      38/50     0.654G    0.08531          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.6it/s 0.2s\n",
            "                   all      0.645          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      39/50     0.662G    0.06242          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.6it/s 0.2s\n",
            "                   all      0.618          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      40/50     0.672G    0.06855          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.4s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 22.7it/s 0.2s\n",
            "                   all      0.553          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      41/50      0.68G    0.07392          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.8s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 22.3it/s 0.2s\n",
            "                   all      0.566          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      42/50     0.688G    0.08508          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 22.3it/s 0.2s\n",
            "                   all      0.592          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      43/50     0.697G    0.05429          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.9s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 25.4it/s 0.2s\n",
            "                   all      0.539          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      44/50     0.705G    0.04137          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 24.0it/s 0.2s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      45/50     0.715G    0.06916          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 52.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 23.1it/s 0.2s\n",
            "                   all      0.579          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      46/50     0.723G    0.05958          4        224: 100% ━━━━━━━━━━━━ 140/140 2.8it/s 50.5s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 19.3it/s 0.3s\n",
            "                   all      0.592          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      47/50     0.732G    0.03177          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.7s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 27.6it/s 0.2s\n",
            "                   all      0.645          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      48/50      0.74G    0.06072          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.6s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 26.2it/s 0.2s\n",
            "                   all      0.658          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      49/50      0.75G    0.04996          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.0s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 21.5it/s 0.2s\n",
            "                   all      0.632          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      50/50     0.758G    0.03218          4        224: 100% ━━━━━━━━━━━━ 140/140 2.7it/s 51.3s\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 30.9it/s 0.2s\n",
            "                   all      0.645          1\n",
            "\n",
            "50 epochs completed in 0.719 hours.\n",
            "Optimizer stripped from /content/runs/classify/train/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from /content/runs/classify/train/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating /content/runs/classify/train/weights/best.pt...\n",
            "Ultralytics 8.3.222 🚀 Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,531,148 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/asset_styles_yolo/train... found 1116 images in 4 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/asset_styles_yolo/val... found 76 images in 4 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/asset_styles_yolo/test... found 44 images in 4 classes ✅ \n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 5/5 3.6it/s 1.4s\n",
            "                   all      0.684          1\n",
            "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/classify/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- CONFIG ---\n",
        "val_dir = \"/content/asset_styles_yolo/val\"  # path to validation dataset\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]  # your actual classes\n",
        "extra_class_name = \"Styles\"  # the 5th class in your dataset to ignore\n",
        "extra_class_idx = len(class_names)  # index of the extra class\n",
        "\n",
        "# Load your trained YOLO classification model\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# --- Iterate through each class folder ---\n",
        "for label_idx, class_name in enumerate(class_names):\n",
        "    class_folder = Path(val_dir) / class_name\n",
        "    if not class_folder.exists():\n",
        "        print(f\"Warning: Folder {class_folder} does not exist.\")\n",
        "        continue\n",
        "\n",
        "    # Iterate through each image in the folder\n",
        "    for img_path in class_folder.iterdir():\n",
        "        if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"]:\n",
        "            continue  # skip unsupported files\n",
        "\n",
        "        # Run prediction\n",
        "        results = model.predict(source=str(img_path), save=False, verbose=False)\n",
        "        pred = results[0].probs  # Probs object\n",
        "        pred_class = int(pred.top1)  # Use top1 attribute instead of argmax\n",
        "\n",
        "        # --- Ignore predictions for the extra \"Styles\" class ---\n",
        "        if pred_class == extra_class_idx:\n",
        "            continue\n",
        "\n",
        "        # Store true and predicted labels\n",
        "        y_true.append(label_idx)\n",
        "        y_pred.append(pred_class)\n",
        "\n",
        "# --- Compute metrics ---\n",
        "precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "\n",
        "# --- Print results ---\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "660BB9J0Qren",
        "outputId": "8e9d52cb-5e31-4e9e-ce81-a4a0eebb13c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3967893217893218\n",
            "Recall: 0.40789473684210525\n",
            "F1 Score: 0.40081914769863125\n",
            "Confusion Matrix:\n",
            " [[ 2 16  0  1]\n",
            " [ 9  2  4  4]\n",
            " [ 1  2 13  3]\n",
            " [ 3  1  1 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIG ---\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]  # actual classes\n",
        "save_dir = Path(\"/content/runs/classify/val\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "save_path = save_dir / \"confusion_matrix.png\"\n",
        "\n",
        "# --- Compute confusion matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "\n",
        "# --- Create figure ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot using seaborn heatmap\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names\n",
        ")\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Save to file\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✅ Confusion matrix saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "cUzhow3ZMWqd",
        "outputId": "3801b953-645d-4d06-eeb5-434845737ebb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbe1JREFUeJzt3Xd8TecfB/DPybqJJDIQEo3E3jFq1Aw1Y9PWJmbtFdSuxIq9ldKKUXRYrVIziFVqhCBCImjtFWSP+/z+8HJ/bhOakJvnuufz9jqvl/ucc8/53JH4es5znqMIIQSIiIiISDXMZAcgIiIiopzFApCIiIhIZVgAEhEREakMC0AiIiIilWEBSERERKQyLACJiIiIVIYFIBEREZHKsAAkIiIiUhkWgEREREQqwwKQiN7q2rVraNy4MRwcHKAoCrZv356t+79x4wYURcGaNWuydb8fsnr16qFevXqyYxCRCWMBSPQBiIqKQr9+/VCkSBFYW1sjd+7cqFWrFhYtWoSEhASDHtvX1xdhYWGYPn061q9fjypVqhj0eDmpR48eUBQFuXPnzvB9vHbtGhRFgaIomDt3bpb3f+fOHfj7+yM0NDQb0hIRZR8L2QGI6O127tyJL774AhqNBt27d0e5cuWQnJyMo0ePYvTo0bh06RJWrlxpkGMnJCTgxIkTmDBhAgYPHmyQY3h4eCAhIQGWlpYG2f9/sbCwQHx8PHbs2IH27dvrrduwYQOsra2RmJj4Tvu+c+cOAgIC4OnpiYoVK2b6eXv37n2n4xERZRYLQCIjFh0djY4dO8LDwwPBwcFwdXXVrRs0aBAiIyOxc+dOgx3/4cOHAABHR0eDHUNRFFhbWxts//9Fo9GgVq1a2LRpU7oCcOPGjWjevDm2bNmSI1ni4+ORK1cuWFlZ5cjxiEi9eAqYyIjNnj0bsbGx+P777/WKv1eKFSuGYcOG6R6npqZi6tSpKFq0KDQaDTw9PTF+/HgkJSXpPc/T0xMtWrTA0aNHUa1aNVhbW6NIkSJYt26dbht/f394eHgAAEaPHg1FUeDp6Qng5anTV39/nb+/PxRF0Wvbt28fateuDUdHR9jZ2aFkyZIYP368bv2bxgAGBwejTp06sLW1haOjI1q3bo3w8PAMjxcZGYkePXrA0dERDg4O6NmzJ+Lj49/8xv5L586d8ccffyAmJkbX9tdff+HatWvo3Llzuu2fPHmCUaNGoXz58rCzs0Pu3Lnh4+OD8+fP67Y5dOgQqlatCgDo2bOn7lTyq9dZr149lCtXDmfOnEHdunWRK1cu3fvy7zGAvr6+sLa2Tvf6mzRpAicnJ9y5cyfTr5WICGABSGTUduzYgSJFiqBmzZqZ2r5Pnz74+uuvUblyZSxYsADe3t4IDAxEx44d020bGRmJzz//HI0aNcK8efPg5OSEHj164NKlSwCAdu3aYcGCBQCATp06Yf369Vi4cGGW8l+6dAktWrRAUlISpkyZgnnz5qFVq1Y4duzYW5+3f/9+NGnSBA8ePIC/vz/8/Pxw/Phx1KpVCzdu3Ei3ffv27fHixQsEBgaiffv2WLNmDQICAjKds127dlAUBVu3btW1bdy4EaVKlULlypXTbX/9+nVs374dLVq0wPz58zF69GiEhYXB29tbV4yVLl0aU6ZMAQB8+eWXWL9+PdavX4+6devq9vP48WP4+PigYsWKWLhwIerXr59hvkWLFiFfvnzw9fVFWloaAODbb7/F3r17sWTJEri5uWX6tRIRAQAEERmlZ8+eCQCidevWmdo+NDRUABB9+vTRax81apQAIIKDg3VtHh4eAoAICQnRtT148EBoNBoxcuRIXVt0dLQAIObMmaO3T19fX+Hh4ZEuw+TJk8Xrv1YWLFggAIiHDx++MferYwQFBenaKlasKFxcXMTjx491befPnxdmZmaie/fu6Y7Xq1cvvX22bdtW5MmT543HfP112NraCiGE+Pzzz0WDBg2EEEKkpaWJAgUKiICAgAzfg8TERJGWlpbudWg0GjFlyhRd219//ZXutb3i7e0tAIgVK1ZkuM7b21uvbc+ePQKAmDZtmrh+/bqws7MTbdq0+c/XSESUEfYAEhmp58+fAwDs7e0ztf2uXbsAAH5+fnrtI0eOBIB0YwXLlCmDOnXq6B7ny5cPJUuWxPXr198587+9Gjv466+/QqvVZuo5d+/eRWhoKHr06AFnZ2ddu5eXFxo1aqR7na/r37+/3uM6derg8ePHuvcwMzp37oxDhw7h3r17CA4Oxr179zI8/Qu8HDdoZvby12daWhoeP36sO7199uzZTB9To9GgZ8+emdq2cePG6NevH6ZMmYJ27drB2toa3377baaPRUT0OhaAREYqd+7cAIAXL15kavubN2/CzMwMxYoV02svUKAAHB0dcfPmTb32QoUKpduHk5MTnj59+o6J0+vQoQNq1aqFPn36IH/+/OjYsSN+/vnntxaDr3KWLFky3brSpUvj0aNHiIuL02v/92txcnICgCy9lmbNmsHe3h4//fQTNmzYgKpVq6Z7L1/RarVYsGABihcvDo1Gg7x58yJfvny4cOECnj17luljFixYMEsXfMydOxfOzs4IDQ3F4sWL4eLikunnEhG9jgUgkZHKnTs33NzccPHixSw9798XYbyJubl5hu1CiHc+xqvxaa/Y2NggJCQE+/fvR7du3XDhwgV06NABjRo1Srft+3if1/KKRqNBu3btsHbtWmzbtu2NvX8AMGPGDPj5+aFu3br44YcfsGfPHuzbtw9ly5bNdE8n8PL9yYpz587hwYMHAICwsLAsPZeI6HUsAImMWIsWLRAVFYUTJ07857YeHh7QarW4du2aXvv9+/cRExOju6I3Ozg5OeldMfvKv3sZAcDMzAwNGjTA/PnzcfnyZUyfPh3BwcE4ePBghvt+lTMiIiLduitXriBv3rywtbV9vxfwBp07d8a5c+fw4sWLDC+ceWXz5s2oX78+vv/+e3Ts2BGNGzdGw4YN070nmS3GMyMuLg49e/ZEmTJl8OWXX2L27Nn466+/sm3/RKQuLACJjNhXX30FW1tb9OnTB/fv30+3PioqCosWLQLw8hQmgHRX6s6fPx8A0Lx582zLVbRoUTx79gwXLlzQtd29exfbtm3T2+7JkyfpnvtqQuR/T03ziqurKypWrIi1a9fqFVQXL17E3r17da/TEOrXr4+pU6di6dKlKFCgwBu3Mzc3T9e7+Msvv+D27dt6ba8K1YyK5awaM2YMbt26hbVr12L+/Pnw9PSEr6/vG99HIqK34UTQREasaNGi2LhxIzp06IDSpUvr3Qnk+PHj+OWXX9CjRw8AQIUKFeDr64uVK1ciJiYG3t7eOHXqFNauXYs2bdq8cYqRd9GxY0eMGTMGbdu2xdChQxEfH4/ly5ejRIkSehdBTJkyBSEhIWjevDk8PDzw4MEDfPPNN/joo49Qu3btN+5/zpw58PHxQY0aNdC7d28kJCRgyZIlcHBwgL+/f7a9jn8zMzPDxIkT/3O7Fi1aYMqUKejZsydq1qyJsLAwbNiwAUWKFNHbrmjRonB0dMSKFStgb28PW1tbVK9eHYULF85SruDgYHzzzTeYPHmyblqaoKAg1KtXD5MmTcLs2bOztD8iIk4DQ/QBuHr1qujbt6/w9PQUVlZWwt7eXtSqVUssWbJEJCYm6rZLSUkRAQEBonDhwsLS0lK4u7uLcePG6W0jxMtpYJo3b57uOP+efuRN08AIIcTevXtFuXLlhJWVlShZsqT44Ycf0k0Dc+DAAdG6dWvh5uYmrKyshJubm+jUqZO4evVqumP8e6qU/fv3i1q1agkbGxuRO3du0bJlS3H58mW9bV4d79/TzAQFBQkAIjo6+o3vqRD608C8yZumgRk5cqRwdXUVNjY2olatWuLEiRMZTt/y66+/ijJlyggLCwu91+nt7S3Kli2b4TFf38/z58+Fh4eHqFy5skhJSdHbbsSIEcLMzEycOHHira+BiOjfFCGyMEqaiIiIiD54HANIREREpDIsAImIiIhUhgUgERERkcqwACQiIiIyIiEhIWjZsiXc3NygKAq2b9+ebpvw8HC0atUKDg4OsLW1RdWqVXHr1q1MH4MFIBEREZERiYuLQ4UKFbBs2bIM10dFRaF27dooVaoUDh06hAsXLmDSpEmwtrbO9DF4FTARERGRkVIUBdu2bUObNm10bR07doSlpSXWr1//zvtlDyARERGRgSUlJeH58+d6y7vcyUer1WLnzp0oUaIEmjRpAhcXF1SvXj3D08RvY5J3Aol+lCg7AuWgMo1GyY5AOejpX0tlR6Ac9CIxVXYEykH57OSVJTaVBht0/2Na50VAQIBe2+TJk7N8d6MHDx4gNjYWM2fOxLRp0zBr1izs3r0b7dq1w8GDB+Ht7Z2p/ZhkAUhERESUJYphT4qOGzcOfn5+em0ajSbL+9FqtQCA1q1bY8SIEQBe3mP9+PHjWLFiBQtAIiIiImOh0WjeqeD7t7x588LCwgJlypTRay9dujSOHj2a6f2wACQiIiJSFNkJMsXKygpVq1ZFRESEXvvVq1fh4eGR6f2wACQiIiIyIrGxsYiMjNQ9jo6ORmhoKJydnVGoUCGMHj0aHTp0QN26dVG/fn3s3r0bO3bswKFDhzJ9DBaARERERAYeA5gVp0+fRv369XWPX40d9PX1xZo1a9C2bVusWLECgYGBGDp0KEqWLIktW7agdu3amT4GC0AiIiIiI1KvXj381zTNvXr1Qq9evd75GCwAiYiIiD6QMYDZxXj6O4mIiIgoR7AHkIiIiMiIxgDmBBaARERERDwFTERERESmjD2ARERERCo7BayuV0tERERE8noAL1y4kOltvby8DJiEiIiIVE9lYwClFYAVK1aEoigQQkD5jzc9LS0th1IRERERmT5pBWB0dLTu7+fOncOoUaMwevRo1KhRAwBw4sQJzJs3D7Nnz5YVkYiIiNRCZWMApRWAHh4eur9/8cUXWLx4MZo1a6Zr8/Lygru7OyZNmoQ2bdpISEhERERkmoziKuCwsDAULlw4XXvhwoVx+fJlCYmIiIhIVVQ2BtAo+jtLly6NwMBAJCcn69qSk5MRGBiI0qVLS0xGREREqqCYGXYxMkbRA7hixQq0bNkSH330ke6K3wsXLkBRFOzYsUNyOiIiIiLTYhQFYLVq1XD9+nVs2LABV65cAQB06NABnTt3hq2treR0REREZPJUdgrYKApAALC1tcWXX34pOwYRERGRyTOak9Lr169H7dq14ebmhps3bwIAFixYgF9//VVyMiIiIjJ5KhsDaBSJli9fDj8/P/j4+ODp06e6iZ+dnJywcOFCueGIiIiITIxRFIBLlizBqlWrMGHCBFhY/P+sdJUqVRAWFiYxGREREakCewBzXnR0NCpVqpSuXaPRIC4uTkIiIiIiItNlFAVg4cKFERoamq599+7dnAeQiIiIDM9MMexiZIziKmA/Pz8MGjQIiYmJEELg1KlT2LRpEwIDA/Hdd9/JjkdERESmzghP0xqSURSAffr0gY2NDSZOnIj4+Hh07twZbm5uWLRoETp27Cg7HhEREZFJMYoCEAC6dOmCLl26ID4+HrGxsXBxcZEdiYiIiNRCZRNBG0V/56effoqYmBgAQK5cuXTF3/Pnz/Hpp59KTEZERERkeoyiB/DQoUNITk5O156YmIgjR45ISERERESqwjGAOefChQu6v1++fBn37t3TPU5LS8Pu3btRsGBBGdGIiIiITJbUArBixYpQFAWKomR4qtfGxgZLliyRkIyIiIhURWVjAKUWgNHR0RBCoEiRIjh16hTy5cunW2dlZQUXFxeYm5tLTEhERERkeqQWgB4eHgAArVYrMwYRERGpHccAynHt2jUcPHgQDx48SFcQfv3115JSERERkSrwFHDOW7VqFQYMGIC8efOiQIECUF77EBRFYQFIRERElI2MogCcNm0apk+fjjFjxsiOQkRERGqkslPARvFqnz59ii+++EJ2DCIiIiJVMIoC8IsvvsDevXtlxyAiIiK1UhTDLkbGKE4BFytWDJMmTcKff/6J8uXLw9LSUm/90KFDJSUjIiIiMj1GUQCuXLkSdnZ2OHz4MA4fPqy3TlEUFoBERERkWCobA2gUBWB0dLTsCERERESqYRQFIBEREZFURjhOz5CkFYB+fn6YOnUqbG1t4efn99Zt58+fn0OpiIiIiEyftALw3LlzSElJ0f39TRSVVeREREQkAccA5oyDBw9m+HciIiKiHKeyAlBdr5aIiIiIjOcikNOnT+Pnn3/GrVu3kJycrLdu69atklIRERGRKqhsyJlR9AD++OOPqFmzJsLDw7Ft2zakpKTg0qVLCA4OhoODg+x4RERERCbFKArAGTNmYMGCBdixYwesrKywaNEiXLlyBe3bt0ehQoVkxzNqP677HkN6d0bbhjXQoXk9BIwdjr9v3pAdi7JJrcpFsXlhP1zfOx0J55aiZT2vdNuULJwfvyzsh3shc/Do+Dwc/WE03As4SUhLhvLjxg3wafQpqlYqjy4dv0DYhQuyI5GBhJ49ja+GD0TrJvVQ++OyCDl4QHYk9VDMDLsYGaNIFBUVhebNmwMArKysEBcXB0VRMGLECKxcuVJyOuMWFnoaLdt1wIKV6xG48FukpqZiwoj+SEyIlx2NsoGtjQZhV29jeOBPGa4v/FFeHFjth6vR99Ck7yJUbR+IwFW7kZiUksNJyVB2/7ELc2cHot/AQfjxl20oWbIUBvTrjcePH8uORgaQkJCAYiVKwm/MRNlRyMQZRQHo5OSEFy9eAAAKFiyIixcvAgBiYmIQH89C5m2mz1+Oxs1bw7NIMRQpXhIjJ0zBg/t3cS0iXHY0ygZ7j11GwDe/47eDGff4BAxuiT1HL2HCol9xPuIfRP/zCDsPh+Hh09gcTkqGsn5tENp93h5t2n6GosWKYeLkAFhbW2P71i2yo5EB1KhVB18OHAbvTxvKjqI+imLYJQtCQkLQsmVLuLm5QVEUbN++/Y3b9u/fH4qiYOHChVk6hlEUgHXr1sW+ffsAAF988QWGDRuGvn37olOnTmjQoIHkdB+W+LiX//Db584tOQkZmqIoaFq7LK7deoDflg3CzQOBCFk3KsPTxPRhSklORvjlS/ikRk1dm5mZGT75pCYunH/z/KlE9GGLi4tDhQoVsGzZsrdut23bNvz5559wc3PL8jGM4irgpUuXIjExEQAwYcIEWFpa4vjx4/jss88wceLbu8GTkpKQlJT0rzYBjUZjsLzGSqvVYsWi2SjjVRGeRYrLjkMG5uJsB3tba4zq2QgBy37HxEXb0bhWGfw4rw+afLkYR89Eyo5I7+lpzFOkpaUhT548eu158uRBdPR1SamITJSBx+llVK9oNJoM6xUfHx/4+Pi8dX+3b9/GkCFDsGfPHt0wuqyQ3gN448YNbNmyBdu2bcPFixdhZmaGsWPH4rfffsO8efPg5PT2weyBgYFwcHDQW5YvmpND6Y3LsnkzcON6FMYFzJYdhXKAmdnLH9/fD4VhyYaDuHD1NuYG7cOuI5fQ9/PaktMREX1gDHwKOKN6JTAw8J2iarVadOvWDaNHj0bZsmXfaR9SewAPHjyIFi1aICEh4WUYCwusXr0aXbt2zfQ+xo0bl+5ewndeiGzN+SFYNm8GTh4Pwdxlq5HPJb/sOJQDHj2NRUpKGsKv39Vrj7h+DzUrFZGUirKTk6MTzM3N013w8fjxY+TNm1dSKiJ6FxnVK+96tnLWrFmwsLDA0KFD3zmP1B7ASZMmoVGjRrh9+zYeP36Mvn374quvvsrSPjQaDXLnzq23qOn0rxACy+bNwPGQYMxavAoF3D6SHYlySEpqGs5cvokSHvoFf3EPF9y6+1RSKspOllZWKF2mLE7+eULXptVqcfLkCXhVqCQxGZHpURTFoEt21StnzpzBokWLsGbNGijvMXm11ALw4sWLmDFjBlxdXeHk5IQ5c+bgwYMHnN4gC5bNm4Hgvbswxn8mbHLZ4snjR3jy+BGSkhJlR6NsYGtjBa8SBeFVoiAAwLNgHniVKKib52/B2v34vEll9GxbE0Xc86J/h7poVrccVv4cIjM2ZaNuvj2xdfPP+G37NlyPisK0Kf5ISEhAm7btZEcjA4iPj8O1iHDdTA537/yDaxHhuHf3juRkZCyOHDmCBw8eoFChQrCwsICFhQVu3ryJkSNHwtPTM9P7UYQQ0s6XmpmZ4d69e3BxcdG12dvb4/z58yhS5N1PYUU/Uk/x07RWhQzb/cZPQePmrXM4jRxlGo2SHcFg6nxcHHu/G5auff1vf+LLyT8AALq3/gSjezVGQRdHXL35ANNW7MTvh8JyOmqOefrXUtkRctymDT9gbdD3ePToIUqWKo0x4yfCyyvjn31T8yIxVXaEHHX29CkM7dczXbtPi9aYEDBDQqKclc9O3sg028+DDLr/uM3pP9fMUBQF27ZtQ5s2bQC8HAJy967+0J8mTZqgW7du6NmzJ0qWLJmp/Uq/CnjPnj16t3vTarU4cOCAbi5AAGjVqpWMaB+E3cfOy45ABnTkzDXYVBr81m3W/fon1v36Zw4lIhk6demKTl0yPzaaPlyVq1TD0TOXZMcgyWJjYxEZ+f+ZHKKjoxEaGgpnZ2cUKlQo3cwAlpaWKFCgQKaLP8AICkBfX990bf369dP9XVEUpKWl5WQkIiIiUpt3H06X7U6fPo369evrHr+6eMTX1xdr1qzJlmNILQC1Wq3MwxMREREZnXr16iErI/Ru3LiR5WNInwcwK5o3b57uvDcRERHR+zL0VcDGRvop4KwICQnRzRlIRERElF2MsUgzpA+qB5CIiIiI3t8H1QNIREREZAjsASQiIiIik8YeQCIiIlI99gASERERkUn7oHoAx48fD2dnZ9kxiIiIyNSoqwNQXgH422+/ZXrbV7eCGzdunKHiEBEREamGtALw1U2NX1EURW/W69fPxfNWcERERGRIHAOYQ7RarW7Zu3cvKlasiD/++AMxMTGIiYnBrl27ULlyZezevVtWRCIiIlIJ3glEguHDh2PFihWoXbu2rq1JkybIlSsXvvzyS4SHh0tMR0RERGRajKIAjIqKgqOjY7p2BweHd7rBMREREVFWGGMvnSEZxTQwVatWhZ+fH+7fv69ru3//PkaPHo1q1apJTEZERERkeoyiB3D16tVo27YtChUqBHd3dwDA33//jeLFi2P79u1ywxEREZHJU1sPoFEUgMWKFcOFCxewb98+XLlyBQBQunRpNGzYUHUfCBEREZGhGUUBCLysvBs3bozGjRvLjkJERERqo7L+JqMpAA8cOIADBw7gwYMH0Gq1eutWr14tKRURERGR6TGKAjAgIABTpkxBlSpV4OrqytO+RERElKPUVnsYRQG4YsUKrFmzBt26dZMdhYiIiFRIbQWgUUwDk5ycjJo1a8qOQURERKQKRlEA9unTBxs3bpQdg4iIiFSKt4KTIDExEStXrsT+/fvh5eUFS0tLvfXz58+XlIyIiIjI9BhFAXjhwgVUrFgRAHDx4kW9dcZYNRMREZGJUVm5YRQF4MGDB2VHICIiIlINoygAiYiIiGRS2xlHoykAT58+jZ9//hm3bt1CcnKy3rqtW7dKSkVERERkeoziKuAff/wRNWvWRHh4OLZt24aUlBRcunQJwcHBcHBwkB2PiIiITJzargI2igJwxowZWLBgAXbs2AErKyssWrQIV65cQfv27VGoUCHZ8YiIiMjEsQCUICoqCs2bNwcAWFlZIS4uDoqiYMSIEVi5cqXkdERERESmxSgKQCcnJ7x48QIAULBgQd1UMDExMYiPj5cZjYiIiFRAbT2ARnERSN26dbFv3z6UL18eX3zxBYYNG4bg4GDs27cPDRo0kB2PiIiIyKQYRQG4dOlSJCYmAgAmTJgAS0tLHD9+HJ999hkmTpwoOR0RERGZPOPrpDMoqQXg8+fPX4awsICdnZ3u8cCBAzFw4ECZ0YiIiIhMltQC0NHRMVPnxdPS0nIgDREREamVMY7TMySpBeDrt4ATQqBZs2b47rvvULBgQYmpiIiIiEyb1ALQ29tb77G5uTk++eQTFClSRFIiIiIiUiP2ABIRERGpjNoKQKOYB5CIiIiIco7R9QCqrQInIiIiI6Cy8kNqAdiuXTu9x4mJiejfvz9sbW312rdu3ZqTsYiIiIhMmtQC0MHBQe9x165dJSUhIiIiNVPbGUipBWBQUJDMwxMRERGpktGNASQiIiLKaWrrAeRVwEREREQqwx5AIiIiUj32ABIRERGpjKIoBl2yIiQkBC1btoSbmxsURcH27dt161JSUjBmzBiUL18etra2cHNzQ/fu3XHnzp0sHYMFIBEREZERiYuLQ4UKFbBs2bJ06+Lj43H27FlMmjQJZ8+exdatWxEREYFWrVpl6Rg8BUxERERkRGeAfXx84OPjk+E6BwcH7Nu3T69t6dKlqFatGm7duoVChQpl6hgsAImIiIgMLCkpCUlJSXptGo0GGo3mvff97NkzKIoCR0fHTD/HJAvAsTvDZUegHHR531zZESgHnbsRIzsCERlIvmKO0o5t6ItAAgMDERAQoNc2efJk+Pv7v9d+ExMTMWbMGHTq1Am5c+fO9PNMsgAkIiIiMibjxo2Dn5+fXtv79v6lpKSgffv2EEJg+fLlWXqulALw+fPnmd42K9UsERER0bswdA9gdp3ufeVV8Xfz5k0EBwdnuV6SUgA6Ojpm+o1OS0szcBoiIiKiD8er4u/atWs4ePAg8uTJk+V9SCkADx48qPv7jRs3MHbsWPTo0QM1atQAAJw4cQJr165FYGCgjHhERESkMsY0D3RsbCwiIyN1j6OjoxEaGgpnZ2e4urri888/x9mzZ/H7778jLS0N9+7dAwA4OzvDysoqU8dQhBDCIOkzqUGDBujTpw86deqk175x40asXLkShw4dyvI+O6w9l03p6EMws3lp2REoB92LSZQdgYgMpIbEi0CKj95t0P1fm9M009seOnQI9evXT9fu6+sLf39/FC5cOMPnHTx4EPXq1cvUMaRfBHLixAmsWLEiXXuVKlXQp08fCYmIiIiI5KlXrx7e1j+XHX130u8E4u7ujlWrVqVr/+677+Du7i4hEREREamNohh2MTbSewAXLFiAzz77DH/88QeqV68OADh16hSuXbuGLVu2SE5HREREZHqk9wA2a9YMV69eRcuWLfHkyRM8efIELVu2xNWrV9GsWTPZ8YiIiEgFFEUx6GJspPcAAi9PA8+YMUN2DCIiIiJVkN4DCABHjhxB165dUbNmTdy+fRsAsH79ehw9elRyMiIiIlIDtY0BlF4AbtmyBU2aNIGNjQ3Onj2ru1Hys2fP2CtIREREZADSC8Bp06ZhxYoVWLVqFSwtLXXttWrVwtmzZyUmIyIiIrUwM1MMuhgb6QVgREQE6tatm67dwcEBMTExOR+IiIiIyMRJLwALFCigd7uTV44ePYoiRYpISERERERqwzGAOaxv374YNmwYTp48CUVRcOfOHWzYsAGjRo3CgAEDZMcjIiIiFeA0MDls7Nix0Gq1aNCgAeLj41G3bl1oNBqMGjUKQ4YMkR2PiIiIyORILwAVRcGECRMwevRoREZGIjY2FmXKlIGdnZ3saERERKQSRthJZ1DSTwGvW7cO4eHhsLKyQpkyZVCtWjXY2dkhMTER69atkx2PiIiIyORILwB79OiBatWqpbvv77Nnz9CzZ09JqYiIiEhN1DYGUHoBCAABAQHo1q0b/P39ZUchIiIiMnnSxwAC0N0Grm3btrh48SLWr18vOxIRERGpiDH20hmS9B7AV2/4J598gpMnTyIyMhI1a9bEjRs35AYjIiIiMlHSC0AhhO7vhQoVwvHjx+Hp6YlGjRpJTEVERERqoraJoKWfAp48ebLelC+5cuXCtm3bMHnyZISEhEhMRkRERGqhtlPARlEAZiQgICCHkxARERGpg5QC8LfffoOPjw8sLS3x22+/vXE7RVHQsmXLHExGREREaqSyDkA5BWCbNm1w7949uLi4oE2bNm/cTlEUpKWl5VwwIiIiIhWQUgBqtdoM/05EREQkg9rGAEq/CpiIiIiIcpaUHsDFixdnetuhQ4caMAkRERERxwDmiAULFmRqO0VRWAASERERZTMpBWB0dLSMwxIRERFliGMAc9jBgwdlRyAiIiKVU9udQKQXgE2bNkXRokUxbdo0/P3337LjEBEREZk86QXg7du3MXjwYGzevBlFihRBkyZN8PPPPyM5OVl2NCIiIlIJRVEMuhgb6QVg3rx5MWLECISGhuLkyZMoUaIEBg4cCDc3NwwdOhTnz5+XHZGIiIjIpEgvAF9XuXJljBs3DoMHD0ZsbCxWr16Njz/+GHXq1MGlS5dkxyMiIiITxTGAEqSkpGDz5s1o1qwZPDw8sGfPHixduhT3799HZGQkPDw88MUXX8iOSURERGQSpEwD87ohQ4Zg06ZNEEKgW7dumD17NsqVK6dbb2tri7lz58LNzU1iSiIiIjJlxjhOz5CkF4CXL1/GkiVL0K5dO2g0mgy3yZs3L6eLISIiIsom0k8BT548GV988UW64i81NRUhISEAAAsLC3h7e8uIR0RERCrAMYA5rH79+njy5Em69mfPnqF+/foSEhEREZHacBqYHCaEyPCNefz4MWxtbSUkIiIiIjJt0sYAtmvXDsDLirtHjx56p4DT0tJw4cIF1KxZU1Y8IiIiUhEj7KQzKGkFoIODA4CXPYD29vawsbHRrbOyssInn3yCvn37yopHREREZLKkFYBBQUEAAE9PT4waNYqne4mIiEgaYxynZ0jSp4GZPHmy3uPDhw8jLi4ONWrUgJOTk6RURERERKZLWgE4a9YsxMbGYurUqQBengr28fHB3r17AQAuLi44cOAAypYtKysiERERqYTaegClXQX8008/6d3xY/PmzQgJCcGRI0fw6NEjVKlSBQEBAbLiEREREZksaT2A0dHR8PLy0j3etWsXPv/8c9SqVQsAMHHiRN7/l4iIiHKEyjoA5fUApqam6k39cuLECb1pX9zc3PDo0SMZ0YiIiEhlOBF0DilatKjuVm+3bt3C1atXUbduXd36f/75B3ny5JEV74NibWEG36oFsfSzsljfpQKm+BRH0Ty5ZMciA/hx3fcY0rsz2jasgQ7N6yFg7HD8ffOG7FiUQ37/eS16NK+ODSvny45COYCfNxmStAJw0KBBGDx4MHr37g0fHx/UqFEDZcqU0a0PDg5GpUqVZMX7oPSrWQjl3eyx7OgNjPotHBfuvMDExsXglMtSdjTKZmGhp9GyXQcsWLkegQu/RWpqKiaM6I/EhHjZ0cjArl+9jEO7t8G9cDHZUSgH8PPOecZ0L+CQkBC0bNkSbm5uUBQF27dv11svhMDXX38NV1dX2NjYoGHDhrh27VqWjiGtAOzbty8WL16MJ0+eoG7dutiyZYve+jt37qBXr16S0n04LM0VVPdwxIbTdxB+Pw73XyRj8/l7uPciCY1L5pUdj7LZ9PnL0bh5a3gWKYYixUti5IQpeHD/Lq5FhMuORgaUmBCPb+d8jZ5DxiOXXW7ZccjA+HlTXFwcKlSogGXLlmW4fvbs2Vi8eDFWrFiBkydPwtbWFk2aNEFiYmKmjyF1HsBevXq9scj75ptv9B7PnDkT/fv3h6OjYw4k+3CYKwrMzRSkpGn12pNTtSjpwsm1TV18XCwAwD43/5EwZeuXz0GFqrVQtlI1/PZTkOw4ZGD8vOUwpnF6Pj4+8PHxyXCdEAILFy7ExIkT0bp1awDAunXrkD9/fmzfvh0dO3bM1DGk9QBm1YwZM/DkyZN07UlJSXj+/LnekpaSLCGhHImpWkQ8iEW7CgXgZGMBRQFqF3FCiXy2cLLhKWBTptVqsWLRbJTxqgjPIsVlxyED+fPwXtyMjMDnPQbKjkI5gJ+36cqoXklKSsryfqKjo3Hv3j00bNhQ1+bg4IDq1avjxIkTmd7PB1MACiEybA8MDISDg4PeEv776hxOJ9eyozehAFjRvjw2dK0In9L5cCz6Kd7wlpGJWDZvBm5cj8K4gNmyo5CBPH54HxtXzke/0QGwstL89xPog8bPWy5DjwHMqF4JDAzMcs579+4BAPLnz6/Xnj9/ft26zJB+K7j3NW7cOPj5+em19fpZXeOh7r9IRsCeSGgszGBjaYaYhFQMq+uJ+7FZ/58FfRiWzZuBk8dDMHfZauRzyf/fT6AP0o3IK3ge8xSTh/rq2rTaNFy9eA4HdmzGd9uPwMzcXGJCyk78vE1bRvXK69Ph5bQPvgDUaDTp3kBzSytJaeRKStUiKVULWytzVChojw2n78iORNlMCIFv5gfieEgwZi/9HgXcPpIdiQyoTIUqmLZso17b9wunosBHHmj+eXcWAyaGn7dcZgYeA5hRvfIuChQoAAC4f/8+XF1dde33799HxYoVM72fD74AJKCCmz0A4M7zJBSw16BrFTfceZaEQ5GPJSej7LZs3gwc3PcHJs9cCJtctnjy+OVk6bZ2dtBorCWno+xmk8sWH3kW1WuzsraBXW6HdO304ePnLZcRXQPyVoULF0aBAgVw4MABXcH3/PlznDx5EgMGDMj0flgAmgAbS3N0+tgNeXJZIjYpDSdvxeDHs3eQxjGAJuf3bT8DAL4a3Fuv3W/8FDRu3lpGJCIiymaxsbGIjIzUPY6OjkZoaCicnZ1RqFAhDB8+HNOmTUPx4sVRuHBhTJo0CW5ubmjTpk2mj/HBFIB16tSBjY2N7BhG6c+bMfjzZozsGJQDdh87LzsCSTZu5nLZESgH8fPOOcY0Dczp06dRv3593eNXYwd9fX2xZs0afPXVV4iLi8OXX36JmJgY1K5dG7t374a1debPBCniTZfX5iCtVovIyEg8ePAAWq3+fHav3x4uszqsPZdd0egDMLN5adkRKAfdi8n8RKdE9GGpUcxR2rGbfHPSoPvfM7C6QfefVdJ7AP/880907twZN2/eTDfVi6IoSEtLk5SMiIiI1MLMeDoAc4T0ArB///6oUqUKdu7cCVdXV6PqgiUiIiIyRdILwGvXrmHz5s0oVow3vCYiIiI51NYBJf1OINWrV9e70oWIiIiIDEt6D+CQIUMwcuRI3Lt3D+XLl4elpf79a728vCQlIyIiIrVQWQeg/ALws88+AwD06tVL16YoCoQQvAiEiIiIcoQCdVWA0gvA6Oho2RGIiIiIVEV6Aejh4SE7AhEREakcp4GRICoqCgsXLkR4eDgAoEyZMhg2bBiKFuW9D4mIiIiym/SrgPfs2YMyZcrg1KlT8PLygpeXF06ePImyZcti3759suMRERGRCiiKYtDF2EjvARw7dixGjBiBmTNnpmsfM2YMGjVqJCkZERERkWmS3gMYHh6O3r17p2vv1asXLl++LCERERERqY2iGHYxNtILwHz58iE0NDRde2hoKFxcXHI+EBEREZGJk34KuG/fvvjyyy9x/fp11KxZEwBw7NgxzJo1C35+fpLTERERkRqYGWM3nQFJLwAnTZoEe3t7zJs3D+PGjQMAuLm5wd/fH0OHDpWcjoiIiNRAZfWf/AJQURSMGDECI0aMwIsXLwAA9vb2klMRERERmS7pBeArDx8+REREBACgVKlSyJs3r+REREREpBbGOFWLIUm/CCQuLg69evWCq6sr6tati7p168LV1RW9e/dGfHy87HhEREREJkd6Aejn54fDhw9jx44diImJQUxMDH799VccPnwYI0eOlB2PiIiIVEBt08BIPwW8ZcsWbN68GfXq1dO1NWvWDDY2Nmjfvj2WL18uLxwRERGRCZJeAMbHxyN//vzp2l1cXHgKmIiIiHKE2qaBkX4KuEaNGpg8eTISExN1bQkJCQgICECNGjUkJiMiIiIyTdJ7ABcuXIimTZvio48+QoUKFQAA58+fh7W1Nfbs2SM5HREREamBuvr/jKAALF++PK5du4YNGzbgypUrAIBOnTqhS5cusLGxkZyOiIiIyPRILwBDQkJQs2ZN9O3bV689NTUVISEhqFu3rqRkREREpBacBzCH1a9fH0+ePEnX/uzZM9SvX19CIiIiIlIbM8Wwi7GRXgAKITKsuh8/fgxbW1sJiYiIiIhMm7RTwO3atQPwssu1R48e0Gg0unVpaWm4cOECatasKSseERERqYjaTgFLKwAdHBwAvOwBtLe317vgw8rKCp988km6cYFERERE9P6kFYBBQUEQQgAAlixZAjs7O1lRiIiISOVU1gEodwygEAIbNmzA3bt3ZcYgIiIiUhWpBaCZmRmKFy+Ox48fy4xBREREKqcoikEXYyP9KuCZM2di9OjRuHjxouwoRERERKogfSLo7t27Iz4+HhUqVICVlVW6u39kNEcgERERUXYyxrn6DEl6Abhw4ULZEYiIiEjljPE0rSFJLwB9fX1lRyAiIiJSFekF4OsSExORnJys15Y7d25JaYiIiEgt1NX/ZwQXgcTFxWHw4MFwcXGBra0tnJyc9BYiIiIiyl7vVAAeOXIEXbt2RY0aNXD79m0AwPr163H06NEs7+urr75CcHAwli9fDo1Gg++++w4BAQFwc3PDunXr3iUeERERUZaYKYpBF2OT5QJwy5YtaNKkCWxsbHDu3DkkJSUBAJ49e4YZM2ZkOcCOHTvwzTff4LPPPoOFhQXq1KmDiRMnYsaMGdiwYUOW90dEREREb5flAnDatGlYsWIFVq1aBUtLS117rVq1cPbs2SwHePLkCYoUKQLg5Xi/V9O+1K5dGyEhIVneHxEREVFWKYphF2OT5QIwIiICdevWTdfu4OCAmJiYLAcoUqQIoqOjAQClSpXCzz//DOBlz6Cjo2OW90dEREREb5flArBAgQKIjIxM13706FFdT15W9OzZE+fPnwcAjB07FsuWLYO1tTWGDx+O0aNHZ3l/RERERFmltlvBZXkamL59+2LYsGFYvXo1FEXBnTt3cOLECYwaNQqTJk3KcoARI0bo/t6wYUNcuXIFZ86cQfHixVG+fPks74+IiIgoq4ywRjOoLBeAY8eOhVarRYMGDRAfH4+6detCo9Fg1KhRGDJkSKb3ExwcjMGDB+PPP//Um+vPw8MDjo6OqFmzJlasWIE6depkNSIRERERvUWWC0BFUTBhwgSMHj0akZGRiI2NRZkyZWBnZ5el/SxcuBB9+/bNcKJnBwcH9OvXD/Pnz2cBSERERAZnjFO1GNI7TwRtZWWFMmXKoFq1alku/gDg/PnzaNq06RvXN27cGGfOnHnXeERERET0BlnuAaxfv/5bBzMGBwdnaj/379/Xm0YmXTALCzx8+DCr8YiIiIiyzFg6ANPS0uDv748ffvgB9+7dg5ubG3r06IGJEydm68UkWS4AK1asqPc4JSUFoaGhuHjxInx9fTO9n4IFC+LixYsoVqxYhusvXLgAV1fXrMYjIiIi+mDNmjULy5cvx9q1a1G2bFmcPn0aPXv2hIODA4YOHZptx8lyAbhgwYIM2/39/REbG5vp/TRr1gyTJk1C06ZNYW1trbcuISEBkydPRosWLbIaj4iIiCjLjGWqluPHj6N169Zo3rw5AMDT0xObNm3CqVOnsvU47zwG8N+6du2K1atXZ3r7iRMn4smTJyhRogRmz56NX3/9Fb/++itmzZqFkiVL4smTJ5gwYUJ2xSMiIiKSJikpCc+fP9dbXt1O93U1a9bEgQMHcPXqVQAvr5k4evQofHx8sjVPlnsA3+TEiRPpevLeJn/+/Dh+/DgGDBiAcePGQQgB4GUF3qRJEyxbtgz58+d/pyxLP+P8gWoSm5gqOwLloFbT98iOQDkoeGpz2RFIJbKtR+wNAgMDERAQoNc2efJk+Pv767WNHTsWz58/R6lSpWBubo60tDRMnz4dXbp0ydY8WS4A27Vrp/dYCIG7d+/i9OnTWZ4I2sPDA7t27cLTp08RGRkJIQSKFy8OJyenrMYiIiIiemeGPgU8btw4+Pn56bVpNJp02/3888/YsGEDNm7ciLJlyyI0NBTDhw+Hm5tblq61+C9ZLgAdHBz0HpuZmaFkyZKYMmUKGjdu/E4hnJycULVq1Xd6LhEREZGx02g0GRZ8/zZ69GiMHTsWHTt2BACUL18eN2/eRGBgoLwCMC0tDT179kT58uXZS0dEREQmw8w4rgFBfHw8zMz0T0ibm5tDq9Vm63GyVACam5ujcePGCA8PZwFIRERElM1atmyJ6dOno1ChQihbtizOnTuH+fPno1evXtl6nCyfAi5XrhyuX7+OwoULZ2sQIiIiIlmMpQdwyZIlmDRpEgYOHIgHDx7Azc0N/fr1w9dff52tx8lyATht2jSMGjUKU6dOxccffwxbW1u99Rnd25eIiIiI/pu9vT0WLlyIhQsXGvQ4mS4Ap0yZgpEjR6JZs2YAgFatWuldMSOEgKIoSEtLy/6URERERAZkLBNB55RMF4ABAQHo378/Dh48aMg8RERERGRgmS4AX03U7O3tbbAwRERERDIYyxjAnJKlMYBq6x4lIiIidVBbiZOlArBEiRL/WQQ+efLkvQIRERERkWFlqQAMCAhIdycQIiIiog+dmcq6ALNUAHbs2BEuLi6GykJEREREOSDTBSDH/xEREZGpMvvvTUxKpl/vq6uAiYiIiOjDlukewOy+CTERERGRsVDbiU619XgSERERqV6W7wVMREREZGp4FTARERGRyqis/uMpYCIiIiK1YQ8gERERqZ7a7gXMHkAiIiIilWEPIBEREame2i4CYQ8gERERkcqwB5CIiIhUT2UdgOwBJCIiIlIb9gASERGR6qntKmAWgERERKR6CtRVAfIUMBEREZHKsAeQiIiIVE9tp4CNogewV69eePHiRbr2uLg49OrVS0IiIiIiItNlFAXg2rVrkZCQkK49ISEB69atk5CIiIiI1MRMMexibKSeAn7+/DmEEBBC4MWLF7C2ttatS0tLw65du+Di4iIxIREREZHpkVoAOjo6QlEUKIqCEiVKpFuvKAoCAgIkJCMiIiI1UVQ2E7TUAvDgwYMQQuDTTz/Fli1b4OzsrFtnZWUFDw8PuLm5SUxIREREZHqkFoDe3t5ITU2Fr68vqlSpAnd3d5lxiIiISKWMcZyeIUm/CMTCwgKbN29GWlqa7ChERESkUopi2MXYSC8AAeDTTz/F4cOHZccgIiIiUgWjmAjax8cHY8eORVhYGD7++GPY2trqrW/VqpWkZERERKQGZsbYTWdARlEADhw4EAAwf/78dOsUReHpYSIiIqJsZBQFoFarlR2BiIiIVIwXgRiRmJgYLF26VHYMIiIiIpNilAXggQMH0LlzZ7i6umLy5Mmy4xAREZGJ41XAkvz999+YMmUKChcujMaNG0NRFGzbtg337t2THY2IiIjIpEgtAFNSUvDLL7+gSZMmKFmyJEJDQzFnzhyYmZlhwoQJaNq0KSwtLWVGJCIiIhUwg2LQxdhIvQikYMGCKFWqFLp27Yoff/wRTk5OAIBOnTrJjEVERERk0qQWgKmpqVAUBYqiwNzcXGYUIiIiUjFjHKdnSFJPAd+5cwdffvklNm3ahAIFCuCzzz7Dtm3boKjtUyAiIiKpzBTDLsZGagFobW2NLl26IDg4GGFhYShdujSGDh2K1NRUTJ8+Hfv27eMk0ERERETZzGiuAi5atCimTZuGmzdvYufOnUhKSkKLFi2QP39+2dGIiIjIxJkpikEXY2M0BeArZmZm8PHxwebNm/HPP/9g/PjxunWbNm1CXFycxHREREREHz6juBXcm+TLlw9+fn66x/369UP16tVRpEgRiamMT+jZ09i4bjUiwi/j8aOHmDF3MerWbyA7FhnAj+u+x7HDB/DPzWhYaTQoU74ieg0YDncPT9nRKBvUKJEPg3xKooKHMwo42aD74qP449xt3frRrcuibfVCcHPOhZRULc7feIIZW8Nw9voTiakpu+z57Rfs+W0zHt6/CwBw9yiCz7v1ReXqtSQnUwcj7KQzKKPrAXwbIYTsCEYpISEBxUqUhN+YibKjkIGFhZ5Gy3YdsGDlegQu/BapqamYMKI/EhPiZUejbJBLY45Lf8dgzA9nMlwfdf8Fxv5wFt6TdqPFjAP4+3E8fhnpjTz2mhxOSoaQJ29+dO07BLOX/4BZ36xHuUpVMftrP/x9I0p2NDJBRt0DSJlTo1Yd1KhVR3YMygHT5y/XezxywhR0bFEf1yLCUb7ix5JSUXY5EHYPB8LefPejrX/e0ns8adM5dK1bBGU+csCR8AeGjkcGVqVmXb3HnXsPwt4dm3H1chjcPYtKSqUexjRO7/bt2xgzZgz++OMPxMfHo1ixYggKCkKVKlWy7RgsAIk+YPFxsQAA+9y5JSehnGZpbobu9YriWXwyLv0dIzsOZbO0tDScOLwfiYkJKFHGS3YcykFPnz5FrVq1UL9+ffzxxx/Ily8frl27prtZRnb54AvApKQkJCUl6belmEOj4SkRMm1arRYrFs1GGa+K8CxSXHYcyiGNKrhiVf8asLGywP1nCfh87mE8iU2WHYuyyc3r1zBhSE8kJyfD2sYGXwXMhbsnx73nBEN3AGZUr2g0mnT1yqxZs+Du7o6goCBdW+HChbM9zwc1BjAjgYGBcHBw0FsWzZslOxaRwS2bNwM3rkdhXMBs2VEoBx0Lf4D6k/ei2fQDCA67h+8G1EBejgE0GW7unpizchMCl61Fk1afY+msyfj7xnXZsVTBzMBLRvVKYGBguhy//fYbqlSpgi+++AIuLi6oVKkSVq1aZZDX+8Hw8PCApaWlXtu4cePw7NkzvWXYyDGSEhLljGXzZuDk8RDMXrIK+Vw4V6aaxCenIfpBLM5cf4zhQX8hTSvQpS57iEyFpaUlXAu6o2iJ0ujSZwg8ipbArq2bZMeibJBRvTJu3Lh0212/fh3Lly9H8eLFsWfPHgwYMABDhw7F2rVrszXPB3UK+OLFi+naMuo+TYpNzalIRDlKCIFv5gfieEgwZi/9HgXcPpIdiSRTFAVWFh/U/+UpC4RWi5QUnuLPCYa+DW1G9UpGtFotqlSpghkzZgAAKlWqhIsXL2LFihXw9fXNtjzSCkAnJ6dMv9lPnnCOq7eJj4/D7b//f3Xg3Tv/4FpEOOxzO6CAq5vEZJTdls2bgYP7/sDkmQthk8sWTx4/AgDY2tlBo7GWnI7el63GAoVd7HSPC+WzRTl3RzyNS8bT2CSMaFkGu8/dwf1nCXC206B3g2JwdbLBb3/9LTE1ZZcN3y1BpWq1kNelABLi43A0eDcunT+DiTOXyo5GOcjV1RVlypTRaytdujS2bNmSrceRVgAuXLhQ1qFNzpXLlzC0X0/d4yXzX44J82nRGhMCZsiKRQbw+7afAQBfDe6t1+43fgoaN28tIxJlowqeTvh17Ke6x9M6VQIA/Hg0GqPWnkYx19wIquUJZzsNnsYm49yNJ2gZGIyIO89lRaZs9OzpUyyZ+TWePnmEXLZ28ChSHBNnLkWFKp/IjqYKxjIJTK1atRAREaHXdvXqVXh4eGTrcRRhgrMrP+QpYFWJTeTnrSbVRv8qOwLloOCpzWVHoBxU/iO7/97IQNadNmxPevcq7pna7q+//kLNmjUREBCA9u3b49SpU+jbty9WrlyJLl26ZFseoxk4EhUVhYkTJ6JTp0548ODlhKZ//PEHLl26JDkZERERmTozRTHokllVq1bFtm3bsGnTJpQrVw5Tp07FwoULs7X4A4ykADx8+DDKly+PkydPYuvWrYiNfTm57fnz5zF58mTJ6YiIiIhyTosWLRAWFobExESEh4ejb9++2X4MoygAx44di2nTpmHfvn2wsrLStX/66af4888/JSYjIiIiNVAMvBgbo5gGJiwsDBs3bkzX7uLigkePHklIRERERGpiRLcCzhFG0QPo6OiIu3fvpms/d+4cChYsKCERERERkekyigKwY8eOGDNmDO7duwdFUaDVanHs2DGMGjUK3bt3lx2PiIiITJyiKAZdjI1RFIAzZsxAqVKl4O7ujtjYWJQpUwZ169ZFzZo1MXHiRNnxiIiIiEyKUYwBtLKywqpVq/D1118jLCwMsbGxqFSpEooXLy47GhEREamAUfSI5SCjeL0HDx4EALi7u6NZs2Zo3769rvj79ttvZUYjIiIiMjlGUQA2bdoUo0ePRkpKiq7t0aNHaNmyJcaOHSsxGREREakBxwBKcPDgQWzbtg1Vq1bF5cuXsXPnTpQrVw7Pnz9HaGio7HhEREREJsUoCsCaNWsiNDQU5cqVQ+XKldG2bVuMGDEChw4dyvabHxMRERH9m9omgjaKAhAArl69itOnT+Ojjz6ChYUFIiIiEB8fLzsWERERqQBPAUswc+ZM1KhRA40aNcLFixdx6tQpnDt3Dl5eXjhx4oTseEREREQmxSimgVm0aBG2b98OHx8fAEC5cuVw6tQpjB8/HvXq1UNSUpLkhERERGTKjKJHLAcZRQEYFhaGvHnz6rVZWlpizpw5aNGihaRURERERKbJKArAfxd/r/P29s7BJERERKRGxjhOz5CkFYDt2rXDmjVrkDt3brRt2/atb/zWrVtzMBkRERGRaZNWADo4OOiKPkdHRyiKAiGErDhERESkYurq/5NYAAYFBSEtLQ2zZs3C1atXkZycjE8//RT+/v6wsbGRFYuIiIjI5Em96GXGjBkYP3487OzsULBgQSxevBiDBg2SGYmIiIhUSFEMuxgbqQXgunXr8M0332DPnj3Yvn07duzYgQ0bNkCr1cqMRURERCpjBsWgi7GRWgDeunULzZo10z1u2LAhFEXBnTt3JKYiIiIiMm1Sp4FJTU2FtbW1XpulpSVSUlIkJSIiIiI1MsbTtIYktQAUQqBHjx7QaDS6tsTERPTv3x+2tra6Nk4DQ0RERJR9pBaAvr6+6dq6du0qIQkRERGpmWKE4/QMSWoBGBQUJPPwRERERKpkFLeCIyIiIpJJbWMApV4FTEREREQ5jz2AREREpHrGOFefIbEAJCIiItXjKWAiIiIiMmnsASQiIiLVYw8gEREREZk09gASERGR6qltImj2ABIRERGpDHsAiYiISPXM1NUByB5AIiIiIrVhDyARERGpntrGALIAJCIiItXjNDBEREREZNLYA0hERESqp7ZTwOwBJCIiIlIZ9gASERGR6nEaGCIiIiIyaewBJCIiItXjGEAiIiIiMmnsASQiIiLV4zyARERERCqjGHh5VzNnzoSiKBg+fPh77CU9FoBERERERuivv/7Ct99+Cy8vr2zfNwtAIiIiUj0zRTHoklWxsbHo0qULVq1aBScnp+x/vdm+RyIiIiLSk5SUhOfPn+stSUlJb9x+0KBBaN68ORo2bGiQPCZ5Eci9mETZESgHFXC0lh2BctDlJZ/JjkA5qFC3NbIjUA5K2NZH2rENfQ1IYGAgAgIC9NomT54Mf3//dNv++OOPOHv2LP766y+D5THJApCIiIjImIwbNw5+fn56bRqNJt12f//9N4YNG4Z9+/bB2tpwHRwsAImIiIgM3AWo0WgyLPj+7cyZM3jw4AEqV66sa0tLS0NISAiWLl2KpKQkmJubv3ceFoBERERERqJBgwYICwvTa+vZsydKlSqFMWPGZEvxB7AAJCIiIjKaW8HZ29ujXLlyem22trbIkydPuvb3wQKQiIiIVE9tdwJhAUhERERkxA4dOpTt+2QBSERERKqnsg5ATgRNREREpDbsASQiIiJSWRcgewCJiIiIVIY9gERERKR6xjINTE5hDyARERGRyrAHkIiIiFRPbfMAsgeQiIiISGXYA0hERESqp7IOQBaARERERGqrAHkKmIiIiEhl2ANIREREqsdpYIiIiIjIpLEHkIiIiFSP08AQERERkUljDyARERGpnso6ANkDSERERKQ27AEkIiIiUlkXoPQewF69euHFixfp2uPi4tCrVy8JiYiIiEhtFAP/MTbSC8C1a9ciISEhXXtCQgLWrVsnIRERERGRaZN2Cvj58+cQQkAIgRcvXsDa2lq3Li0tDbt27YKLi4useERERKQiapsGRloB6OjoCEVRoCgKSpQokW69oigICAiQkIyIiIjItEkrAA8ePAghBD799FNs2bIFzs7OunVWVlbw8PCAm5ubrHhERESkIirrAJRXAHp7eyM1NRW+vr6oUqUK3N3dZUUhIiIiUhWpF4FYWFhg8+bNSEtLkxmDiIiI1E4x8GJkpF8F/Omnn+Lw4cOyYxARERGphvSJoH18fDB27FiEhYXh448/hq2trd76Vq1aSUpGREREamGMc/UZkvQCcODAgQCA+fPnp1unKApPDxMREZHBcRqYHKbVamVHICIiIlIV6WMA3yQmJgZLly6VHYOIiIhUQGXXgBhfAXjgwAF07twZrq6umDx5suw4RERERCbHKArAv//+G1OmTEHhwoXRuHFjKIqCbdu24d69e7KjERERkRqorAtQWgGYkpKCX375BU2aNEHJkiURGhqKOXPmwMzMDBMmTEDTpk1haWkpKx4RERGRyZJ2EUjBggVRqlQpdO3aFT/++COcnJwAAJ06dZIViYiIiFRKbdPASOsBTE1NhaIoUBQF5ubmsmIQERERqY60AvDOnTv48ssvsWnTJhQoUACfffYZtm3bBkVtE/EQERGRdIpi2MXYSCsAra2t0aVLFwQHByMsLAylS5fG0KFDkZqaiunTp2Pfvn2cBJqIiIhyhMquATGOq4CLFi2KadOm4ebNm9i5cyeSkpLQokUL5M+fX3Y0IiIiIpNjFAXgK2ZmZvDx8cHmzZvxzz//YPz48bp1mzZtQlxcnMR0REREZLJU1gVoVAXg6/Llywc/Pz/d4379+uH+/fsSExERERGZBun3As4sIYTsCERERGSiOA0MEREREZm0D6YHkIiIiMhQjHGqFkNiDyARERGRyrAHkIiIiFRPZR2AcnsA09LSEBISgpiYmP/c1sPDA5aWloYPRUREROrDaWByjrm5ORo3boynT5/+57YXL16Eu7t7DqT6sOz57Rf49emAbi3rolvLuhg/uAfOnjwmOxYZUOjZ0/hq+EC0blIPtT8ui5CDB2RHIgPi5226apUpgM3jG+P6952QsK0PWlbzeOO2i/vXQsK2PhjcomwOJiRTJn0MYLly5XD9+nXZMT5YefLmR9e+QzB7+Q+Y9c16lKtUFbO/9sPfN6JkRyMDSUhIQLESJeE3ZqLsKJQD+HmbLltrC4TdeIzhK4+/dbtW1T1QrYQL7jzmzRAMSTHwn8wKDAxE1apVYW9vDxcXF7Rp0wYRERHZ/nqljwGcNm0aRo0ahalTp+Ljjz+Gra2t3vrcuXNLSvZhqFKzrt7jzr0HYe+Ozbh6OQzunkUlpSJDqlGrDmrUqiM7BuUQft6ma+/Zf7D37D9v3cbNORfm96mJllP+wLaJTXIoGcl0+PBhDBo0CFWrVkVqairGjx+Pxo0b4/Lly+lqpPchvQBs1qwZAKBVq1ZQXrsGWwgBRVGQlpYmK9oHJy0tDScO70diYgJKlPGSHYeIiN6DogDfD6+HBb9eQPjfMbLjmDxjmQZm9+7deo/XrFkDFxcXnDlzBnXr1n3Ds7JOegF48ODB93p+UlISkpKS9NqSk1JgpdG8134/JDevX8OEIT2RnJwMaxsbfBUwF+6eRWTHIiKi9zCybQWkpmmx7PdLsqNQNsioXtFoNND8R73y7NkzAICzs3O25pFeAHp7e7/X8wMDAxEQEKDX1n/EOAz0G/9e+/2QuLl7Ys7KTYiPi8WfIfuxdNZkBMxfxSKQiOgDValIHgxqURY1R26XHUU1DN0BmFG9MnnyZPj7+7/xOVqtFsOHD0etWrVQrly5bM0jvQAEgJiYGHz//fcIDw8HAJQtWxa9evWCg4PDfz533Lhx8PPz02u79jDFIDmNlaWlJVwLvrxCumiJ0oiMuIxdWzehn98EycmIiOhd1CpTAC4ONri6qqOuzcLcDDN7VMfgluVQqt9PEtPRu8ioXvmv3r9Bgwbh4sWLOHr0aLbnkV4Anj59Gk2aNIGNjQ2qVasGAJg/fz6mT5+OvXv3onLlym99fkbdp1bPYw2W90MgtFqkpCTLjkFERO9o4+FIBF+4o9e24+um2Hg4EusOXJWUysQZuAswM6d7Xzd48GD8/vvvCAkJwUcffZTteaQXgCNGjECrVq2watUqWFi8jJOamoo+ffpg+PDhCAkJkZzQuG34bgkqVauFvC4FkBAfh6PBu3Hp/BlMnLlUdjQykPj4ONz++5bu8d07/+BaRDjsczuggKubxGRkCPy8TZettQWKFvj/TBee+e3h5emMp7FJ+PtRHJ680B8vlpKmxf2n8bh251lOR1WFrEzVYkhCCAwZMgTbtm3DoUOHULhwYYMcR3oBePr0ab3iDwAsLCzw1VdfoUqVKhKTfRiePX2KJTO/xtMnj5DL1g4eRYpj4sylqFDlE9nRyECuXL6Eof166h4vmT8bAODTojUmBMyQFYsMhJ+36apcNB/2Tmuuezy718vf2+uDr+LLJez8UKtBgwZh48aN+PXXX2Fvb4979+4BABwcHGBjY5Ntx1GEECLb9vYO8ufPj/Xr16Nx48Z67Xv27EH37t1x//79LO8z7B91nwJWmwKO1rIjEJGBFOq2RnYEykEJ2/pIO/atJ0n/vdF7KOScudO/yhvmowkKCkKPHj2yLY/0HsAOHTqgd+/emDt3LmrWrAkAOHbsGEaPHo1OnTpJTkdERESUc3KqX056ATh37lwoioLu3bsjNTUVwMurWgcMGICZM2dKTkdERERqYBwjAHOO9FPAr8THxyMq6uX9a4sWLYpcuXK98754ClhdeAqYyHTxFLC6yDwF/LeBTwG7Z/IUcE4xkx2gV69eePHiBXLlyoXy5cujfPnyyJUrF+Li4tCrVy/Z8YiIiEgFFMWwi7GRXgCuXbsWCQkJ6doTEhKwbt06CYmIiIiITJu0MYDPnz+HEAJCCLx48QLW1v8/jZeWloZdu3bBxcVFVjwiIiJSFSPspjMgaQWgo6MjFEWBoigoUaJEuvWKoqS7Zx4RERGRIRjjaVpDklYAHjx4EEIIfPrpp9iyZQucnZ1166ysrODh4QE3N85yT0RERJTdpBWA3t7eAIDo6GgUKlTojRMfEhERERma2qoQ6ReBBAcHY/Pmzenaf/nlF6xdu1ZCIiIiIiLTJr0ADAwMRN68edO1u7i4YMYM3ueSiIiIDI/TwOSwW7duoXDhwunaPTw8cOvWLQmJiIiIiEyb9ALQxcUFFy5cSNd+/vx55MmTR0IiIiIiUhvFwH+MjfQCsFOnThg6dCgOHjyItLQ0pKWlITg4GMOGDUPHjh1lxyMiIiIyOdKuAn5l6tSpuHHjBho0aAALi5dxtFotunfvzjGARERElDOMr5POoKQXgFZWVvjpp58wdepUnD9/HjY2Nihfvjw8PDxkRyMiIiKVUFn9J78AfMXT0xNCCBQtWlTXE0hERERE2U/6GMD4+Hj07t0buXLlQtmyZXVX/g4ZMgQzZ86UnI6IiIjUgNPA5LBx48bh/PnzOHToEKytrXXtDRs2xE8//SQxGREREZFpkn6udfv27fjpp5/wySef6N0OrmzZsoiKipKYjIiIiNTCGKdqMSTpPYAPHz6Ei4tLuva4uDjeH5iIiIjIAKQXgFWqVMHOnTt1j18Vfd999x1q1KghKxYRERGpiWLgxchIOwV88eJFlCtXDoGBgWjatCkuX76MlJQULFq0CJcvX8bx48dx+PBhWfGIiIiITJa0HkAvLy9Ur14dly9fxrFjx5CamgovLy/s3bsXLi4uOHHiBD7++GNZ8YiIiEhFVNYBKK8H8PDhwwgKCsLIkSOh1Wrx2WefYe7cuahbt66sSERERESqIK0HsE6dOli9ejXu3r2LJUuW4MaNG6hXrx5KlCiBWbNm4d69e7KiERERkcpwHsAcZmtri549e+Lw4cO4evUqvvjiCyxbtgyFChVCq1atZMcjIiIiFVAM/MfYSC8AX1esWDGMHz8eEydOhL29vd7VwURERESUPaRPBP1KSEgIVq9ejS1btsDMzAzt27dH7969ZcciIiIiFTDG07SGJLUAvHPnDtasWYM1a9YgMjISNWvWxOLFi9G+fXvY2trKjEZERERksqQVgD4+Pti/fz/y5s2L7t27o1evXihZsqSsOERERESqIa0AtLS0xObNm9GiRQuYm5vLikFERESkOtIKwN9++03WoYmIiIj0qG0MoFFdBUxEREREhmc0VwETERERyWKMc/UZEgtAIiIiUj2eAiYiIiIik8YeQCIiIlI9lXUAsgeQiIiISG3YA0hERESksi5A9gASERERqQx7AImIiEj11DYNDHsAiYiIiFSGPYBERESkemqbB5AFIBEREameyuo/ngImIiIiUhv2ABIRERGprAuQPYBEREREKsMCkIiIiFRPMfCfrFq2bBk8PT1hbW2N6tWr49SpU9n6elkAEhERERmRn376CX5+fpg8eTLOnj2LChUqoEmTJnjw4EG2HYMFIBEREameohh2yYr58+ejb9++6NmzJ8qUKYMVK1YgV65cWL16dba9XhaARERERAaWlJSE58+f6y1JSUnptktOTsaZM2fQsGFDXZuZmRkaNmyIEydOZFsek7wKuPxHdrIj5LikpCQEBgZi3Lhx0Gg0suOQgfHzVhc1f94J2/rIjpDj1Px5y2Rt4IrIf1ogAgIC9NomT54Mf39/vbZHjx4hLS0N+fPn12vPnz8/rly5km15FCGEyLa9kTTPnz+Hg4MDnj17hty5c8uOQwbGz1td+HmrCz9v05SUlJSux0+j0aQr8u/cuYOCBQvi+PHjqFGjhq79q6++wuHDh3Hy5MlsyWOSPYBERERExiSjYi8jefPmhbm5Oe7fv6/Xfv/+fRQoUCDb8nAMIBEREZGRsLKywscff4wDBw7o2rRaLQ4cOKDXI/i+2ANIREREZET8/Pzg6+uLKlWqoFq1ali4cCHi4uLQs2fPbDsGC0ATodFoMHnyZA4YVgl+3urCz1td+HlThw4d8PDhQ3z99de4d+8eKlasiN27d6e7MOR98CIQIiIiIpXhGEAiIiIilWEBSERERKQyLACJiIiIVIYFoBFSFAXbt2/PkWPVq1cPw4cPz5Fj/duNGzegKApCQ0OlHJ8yz9PTEwsXLtQ9NvR39NChQ1AUBTExMQY7hiH16NEDbdq0ybb9rVmzBo6Ojtm2PyIiFoAZeNMvb2P8R0lRFN3i4OCAWrVqITg4ONPP37p1K6ZOnZrp7T+kou3evXsYMmQIihQpAo1GA3d3d7Rs2VJvbqV3kd3/uGenVatWoUKFCrCzs4OjoyMqVaqEwMBA3frsyn737l34+Pi8934+ZD169ND97FlZWaFYsWKYMmUKUlNTsWjRIqxZs0ZqvoSEBDg7OyNv3rwZ3m80I8b4Oy4ntWzZEk2bNs1w3ZEjR6AoCi5cuPDO+5fx/vJ7QG/CAtAEBAUF4e7duzh27Bjy5s2LFi1a4Pr165l6rrOzM+zt7Q2cMOfduHEDH3/8MYKDgzFnzhyEhYVh9+7dqF+/PgYNGvRO+0xLS4NWq83mpNln9erVGD58OIYOHYrQ0FAcO3YMX331FWJjY7P9WAUKFOAUFQCaNm2Ku3fv4tq1axg5ciT8/f0xZ84cODg4SO+x27JlC8qWLYtSpUplqrc2JSXF8KGMXO/evbFv3z78888/6dYFBQWhSpUq8PLykpBMnxACqampmdqW3wN6I0Hp+Pr6itatW6drP3jwoAAgnj59Kh49eiQ6duwo3NzchI2NjShXrpzYuHGj3vbe3t5iyJAhYvTo0cLJyUnkz59fTJ48WW+bq1evijp16giNRiNKly4t9u7dKwCIbdu2ZSrrv7e9ffu2ACBWrFiR6YzDhg3TPfbw8BDTp08XPXv2FHZ2dsLd3V18++23esd7ffH29tatW7VqlShVqpTQaDSiZMmSYtmyZXrHOnnypKhYsaLQaDTi448/Flu3bhUAxLlz5zL1WrPCx8dHFCxYUMTGxqZb9/TpUyGEEPPmzRPlypUTuXLlEh999JEYMGCAePHihW67oKAg4eDgIH799VdRunRpYW5uLnx9fdO9BwcPHhRCCHHhwgVRv359YW1tLZydnUXfvn319peWliYCAgJEwYIFhZWVlahQoYL4448/dOujo6MFALFlyxZRr149YWNjI7y8vMTx48cz9Zpbt24tevTo8cb1kydPzjB7/fr1xaBBg/S2ffDggbC0tBT79+8XQrz8XixYsEC3/vXvXUb7BSCCgoJ0r3vGjBnC09NTWFtbCy8vL/HLL7/oHW/nzp2iePHiwtraWtSrV08EBQXpftaMVUa/Jxo1aiQ++eQTvXUPHjwQ+fPnF9OnT9dtd+zYMb33NzExUYwcOVK4ubmJXLlyiWrVqum+V0L8/7uYFfXq1RMrVqwQy5cvF40aNUq3HoD45ptvRMuWLUWuXLky/G77+vpm6ZgfupSUFJE/f34xdepUvfYXL14IOzs7sXz5cnHkyBFRu3ZtYW1tLT766CMxZMgQvd8ziYmJ4quvvhIfffSRsLKyEkWLFhXfffed7uc7o/c3MTFRDBkyROTLl09oNBpRq1YtcerUKd0+X/3bs2vXLlG5cmVhaWmp9/14G34P6E1YAGYgMwXgP//8I+bMmSPOnTsnoqKixOLFi4W5ubk4efKkbntvb2+RO3du4e/vL65evSrWrl0rFEURe/fuFUK8/IexXLlyokGDBiI0NFQcPnxYVKpU6b0KwCdPnggAYvHixZnO+O8C0NnZWSxbtkxcu3ZNBAYGCjMzM3HlyhUhhBCnTp0SAMT+/fvF3bt3xePHj4UQQvzwww/C1dVVbNmyRVy/fl1s2bJFODs7izVr1gghXv4CzZcvn+jcubO4ePGi2LFjhyhSpIhBCsDHjx8LRVHEjBkz3rrdggULRHBwsIiOjhYHDhwQJUuWFAMGDNCtDwoKEpaWlqJmzZri2LFj4sqVK+LZs2eiffv2omnTpuLu3bvi7t27IikpScTGxgpXV1fRrl07ERYWJg4cOCAKFy6s94tz/vz5Infu3GLTpk3iypUr4quvvhKWlpbi6tWrQoj/F4ClSpUSv//+u4iIiBCff/658PDwECkpKf/5uvv16ydKlSolbty4keH6Fy9eZJh9w4YNwsnJSSQmJupl9fT0FFqtVgjx9gLwxYsXuv3dvXtXzJ07V+TKlUuEhYUJIYSYNm2aKFWqlNi9e7eIiooSQUFBQqPRiEOHDgkhhLh165bQaDTCz89PXLlyRfzwww8if/78H2QB2KpVK1G5cuV063bu3CksLS3FX3/9JZ4/fy6KFCkiRowYoVvfp08fUbNmTRESEiIiIyPFnDlzhEaj0X03sloARkZGCo1GI548eSIeP34srK2t030vAAgXFxexevVqERUVJW7cuCG2bNkiAIiIiAhx9+5dERMTk+X35UM3evRoUbRoUd13XwghVq9eLWxsbERoaKiwtbUVCxYsEFevXhXHjh0TlSpV0vuPV/v27YW7u7vYunWriIqKEvv37xc//vijSE1NfeP7O3ToUOHm5iZ27dolLl26JHx9fYWTk5Pu9+urf3u8vLzE3r17RWRkpG7d2/B7QG/DAjADvr6+wtzcXNja2uot1tbWb/1HqXnz5mLkyJG6x97e3qJ27dp621StWlWMGTNGCCHEnj17hIWFhbh9+7Zu/R9//PHOBWBcXJwYOHCgMDc3F+fPn890xn8XgF27dtU91mq1wsXFRSxfvlwI8f8i5d9FW9GiRdP1Lk6dOlXUqFFDCCHEt99+K/LkySMSEhJ065cvX26QAvDkyZMCgNi6dWuWnvfLL7+IPHny6B6/6oUKDQ3V2y6jf/hXrlwpnJyc9HoCdu7cKczMzMS9e/eEEEK4ubnp9QIJ8fL7MHDgQCHE/9/b7777Trf+0qVLAoAIDw//z/x37twRn3zyiQAgSpQoIXx9fcVPP/0k0tLS3po9ISFBODk5iZ9++knX5uXlJfz9/XWP31YAvu7EiRPC2tpat6/ExESRK1eudL2YvXv3Fp06dRJCCDFu3DhRpkwZvfVjxoz5oApArVYr9u3bJzQajRg1alSG7/PAgQNFiRIlROfOnUX58uV1BffNmzeFubm53u8BIYRo0KCBGDdunBAi6wXg+PHjRZs2bXSPW7dune7sAwAxfPhwvbbX/5OrVuHh4Xo9+0IIUadOHdG1a1fRu3dv8eWXX+ptf+TIEWFmZiYSEhJERESEACD27duX4b4zen9jY2OFpaWl2LBhg64tOTlZuLm5idmzZ+s9b/v27Vl6Lfwe0NtwDOAb1K9fH6GhoXrLd999p1uflpaGqVOnonz58nB2doadnR327NmDW7du6e3n3+NFXF1d8eDBAwBAeHg43N3d4ebmplv/Ljd67tSpE+zs7GBvb48tW7bg+++/h5eXV6Yz/tvrmRVFQYECBXSZMxIXF4eoqCj07t0bdnZ2umXatGmIiorSvVYvLy9YW1u/12vNDJHJm9vs378fDRo0QMGCBWFvb49u3brh8ePHiI+P121jZWWVqTE/4eHhqFChAmxtbXVttWrVglarRUREBJ4/f447d+6gVq1aes+rVasWwsPD9dpeP56rqysAvPX9f33bEydOICwsDMOGDUNqaip8fX3RtGnTt45dtLa2Rrdu3bB69WoAwNmzZ3Hx4kX06NHjP4/5ulu3bqFNmzYYNWoU2rdvDwCIjIxEfHw8GjVqpPfdWLdund53o3r16nr7MtR3I7v9/vvvsLOzg7W1NXx8fNChQwf4+/tnuO3cuXORmpqKX375BRs2bNCNoQwLC0NaWhpKlCih9x4dPnxY9x5lRVpaGtauXYuuXbvq2rp27Yo1a9ak+x5UqVIly/s3daVKlULNmjV1Pw+RkZE4cuQIevfujfPnz2PNmjV6n1OTJk2g1WoRHR2N0NBQmJubw9vbO9PHi4qKQkpKit7vBktLS1SrVi3d74asfF78HtB/4b2A38DW1hbFihXTa3t9YPCcOXOwaNEiLFy4EOXLl4etrS2GDx+O5ORkvedYWlrqPVYUJdsvJFiwYAEaNmwIBwcH5MuXL8sZ/y2rmV9dZLBq1ap0/5Cbm5tn9eW8t+LFi0NRFFy5cuWN29y4cQMtWrTAgAEDMH36dDg7O+Po0aPo3bs3kpOTkStXLgCAjY0NFEXJqegA9N//V8fOynemXLlyKFeuHAYOHIj+/fujTp06OHz4MOrXr//G5/Tp0wcVK1bEP//8g6CgIHz66afw8PDI9DHj4uLQqlUr1KhRA1OmTNG1v/pu7Ny5EwULFtR7jilcRFK/fn0sX74cVlZWcHNzg4XFm3+lRkVF4c6dO9Bqtbhx4wbKly8P4OV7ZG5ujjNnzqT7ebGzs8typj179uD27dvo0KGDXntaWhoOHDiARo0a6dpe/w8L/V/v3r0xZMgQLFu2DEFBQShatCi8vb0RGxuLfv36YejQoemeU6hQIURGRho0V1Y+L34P6L+wB/AdHTt2DK1bt0bXrl1RoUIFFClSBFevXs3SPkqXLo2///4bd+/e1bX9+eefWc5SoEABFCtWTK/4y66M/2ZlZQXg5S+RV/Lnzw83Nzdcv34dxYoV01sKFy4M4OVrvXDhAhITE3XPe5fXmhnOzs5o0qQJli1bhri4uHTrY2JicObMGWi1WsybNw+ffPIJSpQogTt37mRq/1ZWVnqvH3j5+s6fP693vGPHjsHMzAwlS5ZE7ty54ebmhmPHjuk979ixYyhTpsw7vMrMebXvV7kyyg4A5cuXR5UqVbBq1Sps3LgRvXr1yvQxhBDo2rUrtFot1q9fr1cwlylTBhqNBrdu3Ur33XB3dwfw8r07deqU3j4N9d3Ibq/+o1ioUKG3Fn/Jycno2rUrOnTogKlTp6JPnz66Xt1KlSohLS0NDx48SPceFShQIMuZvv/+e3Ts2DHdGYyOHTvi+++/f+tzM/r5VqP27dvDzMwMGzduxLp169CrVy8oioLKlSvj8uXL6T6nYsWKwcrKCuXLl4dWq8Xhw4cz3G9G72/RokVhZWWl97shJSUFf/3113v9buD3gP4LC8B3VLx4cezbtw/Hjx9HeHg4+vXrh/v372dpHw0bNkSJEiXg6+uL8+fP48iRI5gwYYJRZfw3FxcX2NjYYPfu3bh//z6ePXsGAAgICEBgYCAWL16Mq1evIiwsDEFBQZg/fz4AoHPnzlAUBX379sXly5exa9cuzJ07971f45ssW7YMaWlpqFatGrZs2YJr164hPDwcixcvRo0aNVCsWDGkpKRgyZIluH79OtavX48VK1Zkat+enp64cOECIiIi8OjRI6SkpKBLly6wtraGr68vLl68iIMHD2LIkCHo1q0b8ufPDwAYPXo0Zs2ahZ9++gkREREYO3YsQkNDMWzYsGx5zQMGDMDUqVNx7Ngx3Lx5E3/++Se6d++OfPny6U6pZpT9lT59+mDmzJkQQqBt27aZPq6/vz/279+Pb7/9FrGxsbh37x7u3buHhIQE2NvbY9SoURgxYgTWrl2LqKgonD17FkuWLMHatWsBAP3798e1a9cwevRoREREYOPGjdLn0MtuEyZMwLNnz7B48WKMGTMGJUqU0BXZJUqUQJcuXdC9e3ds3boV0dHROHXqFAIDA7Fz584sHefhw4fYsWMHfH19dT3Br5bu3btj+/btePLkyRuf7+HhAUVR8Pvvv+Phw4cGmULoQ2BnZ4cOHTpg3LhxuHv3rm44xJgxY3D8+HEMHjwYoaGhuHbtGn799VcMHjwYwMufL19fX/Tq1Qvbt29HdHQ0Dh06hJ9//hlAxu+vra0tBgwYgNGjR2P37t24fPky+vbti/j4ePTu3fud8vN7QJkieQyiUcrMVcCPHz8WrVu3FnZ2dsLFxUVMnDhRdO/eXe95/77AQoiXg3BfvzI0IiJC1K5dW1hZWYkSJUqI3bt3v9dVwK97l4z/HuwvhBAVKlTQGzi8atUq4e7uLszMzPSmgdmwYYOoWLGisLKyEk5OTqJu3bp6F2KcOHFCVKhQQVhZWYmKFSvqrjQzxDQwQry8KGLQoEHCw8NDWFlZiYIFC4pWrVrpBnfPnz9fuLq6ChsbG9GkSROxbt06vYHPbxp4/+DBA9GoUSNhZ2eX5Wlg/P39RcGCBYWlpeUbp4F5/f14+vRpugHpb7J582bRrFkz4erqKqysrISbm5v47LPPxIULF/4zuxAvr+bNlSuX7qKU173tIhBvb++3TgOj1WrFwoULRcmSJYWlpaXIly+faNKkiTh8+LBufzt27BDFihUTGo1G1KlTR6xevdroB6G/6ffEv9cdPHhQWFhYiCNHjujWR0dHi9y5c4tvvvlGCPFy0P/XX38tPD09haWlpXB1dRVt27bVfXaZvQhk7ty5wtHRUSQnJ6dbl5SUJBwdHcWiRYuEEG/+3TFlyhRRoEABoSiKqqf/OH78uAAgmjVrptd+6tQp3c+Qra2t8PLy0ru4KyEhQYwYMUL3c1isWDGxevVq3fqM3t+EhAQxZMgQkTdv3rdOA5PZnwd+DygzFCEyOWKeiEzajRs3ULRoUfz111+oXLmy7DhERGRALACJVC4lJQWPHz/GqFGjEB0dnW6cIhERmR6OATRiM2bM0Jtu4PVF7fdhVSMfH583fh9mzJjxzvs9duwYXF1d8ddff2V6HCTJU7Zs2Td+DzZs2CA7HuUQfg/ofbEH0Ig9efLkjQN1bWxs0k2rQabt9u3bSEhIyHCds7MznJ2dczgRyXDz5s033q81f/78Jnlvb0qP3wN6XywAiYiIiFSGp4CJiIiIVIYFIBEREZHKsAAkIiIiUhkWgEREREQqwwKQiIxWjx490KZNG93jevXqYfjw4Tme49ChQ1AUBTExMTl+bCIiQ2ABSERZ1qNHDyiKAkVRYGVlhWLFimHKlClITU016HG3bt2KqVOnZmpbFm1ERG9mITsAEX2YmjZtiqCgICQlJWHXrl0YNGgQLC0tMW7cOL3tkpOTYWVllS3H5FyHRETZgz2ARPRONBoNChQoAA8PDwwYMAANGzbEb7/9pjttO336dLi5uaFkyZIAgL///hvt27eHo6MjnJ2d0bp1a9y4cUO3v7S0NPj5+cHR0RF58uTBV199hX9PU/rvU8BJSUkYM2YM3N3dodFoUKxYMXz//fe4ceMG6tevDwBwcnKCoijo0aMHAECr1SIwMBCFCxeGjY0NKlSogM2bN+sdZ9euXShRogRsbGxQv359vZxERKaABSARZQsbGxskJycDAA4cOICIiAjs27cPv//+O1JSUtCkSRPY29vjyJEjOHbsGOzs7NC0aVPdc+bNm4c1a9Zg9erVOHr0KJ48eYJt27a99Zjdu3fHpk2bsHjxYoSHh+Pbb7+FnZ0d3N3dsWXLFgBAREQE7t69i0WLFgEAAgMDsW7dOqxYsQKXLl3CiBEj0LVrVxw+fBjAy0K1Xbt2aNmyJUJDQ9GnTx+MHTvWUG8bEZEUPAVMRO9FCIEDBw5gz549GDJkCB4+fAhbW1t89913ulO/P/zwA7RaLb777jsoigIACAoKgqOjIw4dOoTGjRtj4cKFGDduHNq1awcAWLFiBfbs2fPG4169ehU///wz9u3bh4YNGwIAihQpolv/6nSxi4sLHB0dAbzsMZwxYwb279+PGjVq6J5z9OhRfPvtt/D29sby5ctRtGhRzJs3DwBQsmRJhIWFYdasWdn4rhERycUCkIjeye+//w47OzukpKRAq9Wic+fO8Pf3x6BBg1C+fHm9cX/nz59HZGRkuvuTJiYmIioqCs+ePcPdu3dRvXp13ToLCwtUqVIl3WngV0JDQ2Fubg5vb+9MZ46MjER8fDwaNWqk156cnIxKlSoBAMLDw/VyANAVi0REpoIFIBG9k/r162P58uWwsrKCm5sbLCz+/+vE1tZWb9vY2Fh8/PHH2LBhQ7r95MuX752Ob2Njk+XnxMbGAgB27tyJggUL6q3TaDTvlIOI6EPEApCI3omtrS2KFSuWqW0rV66Mn376CS4uLsidO3eG27i6uuLkyZOoW7cuACA1NRVnzpxB5cqVM9y+fPny0Gq1OHz4sO4U8Ote9UCmpaXp2sqUKQONRoNbt269seewdOnS+O233/Ta/vzzz/9+kUREHxBeBEJEBtelSxfkzZsXrVu3xpEjRxAdHY1Dhw5h6NCh+OeffwAAw4YNw8yZM7F9+3ZcuXIFAwcOfOscfp6envD19UWvXr2wfft23T5//vlnAICHhwcURcHvv/+Ohw8fIjY2Fvb29hg1ahRGjBiBtWvXIioqCmfPnsWSJUuwdu1aAED//v1x7do1jB49GhEREdi4cSPWrFlj6LeIiChHsQAkIoPLlSsXQkJCUKhQIbRr1w6lS5dG7969kZiYqOsRHDlyJLp16wZfX1/UqFED9vb2aNu27Vv3u3z5cnz++ecYOHAgSpUqhb59+yIuLg4AULBgQQQEBGDs2LHInz8/Bg8eDACYOnUqJk2ahMDAQJQuXRpNmzbFzp07UbhwYQBAoUKFsGXLFmzfvh0VKlTAihUrMGPGDAO+O0REOU8RbxphTUREREQmiT2ARERERCrDApCIiIhIZVgAEhEREakMC0AiIiIilWEBSERERKQyLACJiIiIVIYFIBEREZHKsAAkIiIiUhkWgEREREQqwwKQiIiISGVYABIRERGpzP8A5LFud/mL8gEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Confusion matrix saved to /content/runs/classify/val/confusion_matrix.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "val_dir = \"/content/asset_styles_yolo/val\"\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]\n",
        "\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for label_idx, class_name in enumerate(class_names):\n",
        "    class_folder = Path(val_dir) / class_name\n",
        "    if not class_folder.exists():\n",
        "        continue\n",
        "    for img_path in class_folder.iterdir():\n",
        "        if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"]:\n",
        "            continue\n",
        "        results = model.predict(source=str(img_path), save=False, verbose=False)\n",
        "        pred_class = results[0].probs.top1  # Top-1 predicted class index\n",
        "        y_true.append(label_idx)\n",
        "        y_pred.append(pred_class)\n",
        "\n",
        "# --- Filter out predictions of extra \"Styles\" class if using nc=5 ---\n",
        "y_true_filtered = []\n",
        "y_pred_filtered = []\n",
        "for t, p in zip(y_true, y_pred):\n",
        "    if p < len(class_names):  # Only keep 0-3\n",
        "        y_true_filtered.append(t)\n",
        "        y_pred_filtered.append(p)"
      ],
      "metadata": {
        "id": "GyDwqJJEqiU8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Compute Top-1 Accuracy manually ---\n",
        "top1_accuracy = np.mean(np.array(y_true_filtered) == np.array(y_pred_filtered))\n",
        "\n",
        "metrics_dict = {\n",
        "    \"Top-1 Accuracy\": top1_accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1 Score\": f1\n",
        "}\n",
        "\n",
        "# Save to CSV\n",
        "save_path = \"/content/classification_metrics.csv\"\n",
        "pd.DataFrame([metrics_dict]).to_csv(save_path, index=False)\n",
        "print(f\"✅ Classification metrics successfully saved to: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYUJqzkTMYdU",
        "outputId": "c2f38379-c17f-4e5d-e97d-9559316e74cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Classification metrics successfully saved to: /content/classification_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "# Load your trained model (use the best weights)\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Path to your test image\n",
        "test_image = \"/content/Styles_unzipped/Styles/Pixel Art/Characters/pixel_char_06.png\"\n",
        "# Run prediction\n",
        "results = model.predict(source=test_image, show=True)\n",
        "\n",
        "# Print predicted class\n",
        "print(\"Predicted class:\", results[0].names[results[0].probs.top1])\n",
        "print(\"Confidence:\", results[0].probs.top1conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKA26iWlNLLf",
        "outputId": "bb95d498-a85c-4efd-be89-11ffda5d457c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/Styles_unzipped/Styles/Pixel Art/Characters/pixel_char_06.png: 224x224 Pixel_Art 0.56, Vector_Art 0.39, Cartoon_Stylized 0.04, Hand_Painted 0.01, 3.9ms\n",
            "Speed: 4.7ms preprocess, 3.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Predicted class: Pixel_Art\n",
            "Confidence: tensor(0.5628, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "# Load trained model\n",
        "model = YOLO(\"/content/runs/classify/train/weights/best.pt\")\n",
        "\n",
        "# Path to test image\n",
        "test_image = \"/content/Styles_unzipped/Styles/Pixel Art/Characters/pixel_char_06.png\"\n",
        "\n",
        "# Run prediction\n",
        "results = model.predict(source=test_image, verbose=False)\n",
        "\n",
        "# Get top-1 predicted index and confidence\n",
        "top1_idx = results[0].probs.top1\n",
        "top1_conf = results[0].probs.top1conf\n",
        "\n",
        "# Map index to class name\n",
        "# Filter out the 5th class if it exists\n",
        "class_names = [\"Hand_Painted\", \"Cartoon_Stylized\", \"Pixel_Art\", \"Vector_Art\"]\n",
        "if top1_idx >= len(class_names):\n",
        "    predicted_class = \"Unknown / Extra Class\"\n",
        "else:\n",
        "    predicted_class = class_names[top1_idx]\n",
        "\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Confidence: {top1_conf:.4f}\")"
      ],
      "metadata": {
        "id": "IacDaTFmWkyx",
        "outputId": "e983fd4f-243a-488a-9232-e50b06e86d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Pixel_Art\n",
            "Confidence: 0.5628\n"
          ]
        }
      ]
    }
  ]
}